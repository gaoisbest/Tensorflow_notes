{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is part 2 of text classification based on Recurrent Neural Networks (RNNs).\n",
    "\n",
    "### Core concepts in part 2:\n",
    "- **Multiple RNNs layers** with **`tf.nn.dynamic_rnn`**, which just copy the cell state through and output zero-tensor after real sequence length.\n",
    "- **Variable sequence length**\n",
    "    - For model input, to achieve minimal padding, I first sort the data, then pad them to same sequence length in each batch. Note:**The less the padding, the fast the training is** [1].\n",
    "    - For model output, the hidden state at the actual sequence length step should be extracted.\n",
    "- **Exploding and vanishing gradients**\n",
    "\n",
    "### Variable sequence length\n",
    "- **Input**:\n",
    "    - Passing actual sequence lengths to `sequence_length` of `tf.nn.dynamic_rnn` via `actual_seq_len = tf.cast(tf.reduce_sum(tf.sign(self.input_X), 1), tf.int32)` [1].\n",
    "- **Output**:\n",
    "    - We should extract the hidden state at last actual step. This can be down by the following codes [2]:\n",
    "    ```\n",
    "    batch_range = tf.range(tf.shape(outputs)[0])\n",
    "    indices = tf.stack([batch_range, actual_batch_len - 1], axis=1)\n",
    "    last_output = tf.gather_nd(outputs, indices)\n",
    "    ```\n",
    "- **Trick**\n",
    "    - **Bucketing** can be used to **accelerate** the training process, but do not have the effect on the model accuracy. Each bucket has a corresponding `placeholder`, which corresponds to a sub-graph, and they share other parts of whole computation graph [3]. Not implemented yet. \n",
    "\n",
    "### Exploding and vanishing gradients\n",
    "- **Vanishing gradients**\n",
    "    - Appearance: the weights of last layers change a lot more than those at the beginning layers.\n",
    "    - Why [LSTM / GRU prevents vanishing gradients](https://www.quora.com/How-does-LSTM-help-prevent-the-vanishing-and-exploding-gradient-problem-in-a-recurrent-neural-network) ? \n",
    "\n",
    "- **Exploding gradients**\n",
    "    - `tf.clip_by_global_norm` for avoiding exploding gradients.\n",
    "    ```\n",
    "    variables = tf.trainable_variables()\n",
    "    gradients = tf.gradients(ys=self.cost, xs=variables)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm=self.clip_norm)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    optimize = optimizer.apply_gradients(grads_and_vars=zip(clipped_gradients, variables), global_step=self.global_step)\n",
    "    ```\n",
    "\n",
    "\n",
    "References:  \n",
    "[1] https://danijar.com/variable-sequence-lengths-in-tensorflow/  \n",
    "[2] https://stackoverflow.com/questions/36817596/get-last-output-of-dynamic-rnn-in-tensorflow  \n",
    "[3] https://www.zhihu.com/question/52200883/answer/136317118  \n",
    "[4] https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class `DataGenerator` is used to read input files, convert words to index and generate batch training or testing data. \n",
    "\n",
    "Two extra word are introduced, `PAD` for padding shorter sequences and `OOV` for representing out-of-vocabulary words.\n",
    "\n",
    "Sorting data then pad them in each batch to achieve minimal padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    \"\"\"\n",
    "    reading each training and testing files, and generating batch data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        self.folder_path = args.FOLDER_PATH\n",
    "        self.batch_size = args.BATCH_SIZE\n",
    "        self.vocab_size = args.VOCAB_SIZE\n",
    "        self.num_epoch = args.NUM_EPOCH\n",
    "        self.read_build_input()\n",
    "        self.label_dict = {0:'auto', 1:'business', 2:'IT', 3:'health', 4:'sports', 5:'yule'}\n",
    "        \n",
    "        \n",
    "    def one_hot_encode(self, x, n_classes=6):\n",
    "        return np.eye(n_classes)[[x]][0]\n",
    "\n",
    "    def read_build_input(self):\n",
    "        training_src = []\n",
    "        testing_src = []\n",
    "        training_article_len = []\n",
    "\n",
    "        for cur_category in range(1, 7):\n",
    "            \n",
    "            print('parsing file >>>>>>>>>>>>>>> ', cur_category)\n",
    "            print('-'*100)\n",
    "            \n",
    "            training_input_file = codecs.open(filename=os.path.join(self.folder_path, 'training_' + str(cur_category) + '.cs'), mode='r', encoding='utf-8')\n",
    "            for tmp_line in training_input_file:\n",
    "                #if len(tmp_line.split()) < 50:\n",
    "                #    training_src.append((tmp_line.split(), cur_category-1))\n",
    "                #    training_article_len.append(len(tmp_line.split()))\n",
    "\n",
    "                training_src.append((tmp_line.split(), cur_category-1))\n",
    "                training_article_len.append(len(tmp_line.split()))\n",
    "\n",
    "            testing_input_file = codecs.open(filename=os.path.join(self.folder_path, 'testing_' + str(cur_category) + '.cs'), mode='r', encoding='utf-8')\n",
    "            for tmp_line in testing_input_file:\n",
    "                #if len(tmp_line.split()) < 50:\n",
    "                #    testing_src.append((tmp_line.split(), cur_category-1))\n",
    "\n",
    "                testing_src.append((tmp_line.split(), cur_category-1))\n",
    "\n",
    "        print('='*100)\n",
    "        print('Size of training data:', len(training_src))\n",
    "        print('Size of testing data:', len(testing_src))\n",
    "            \n",
    "        self.TRAINING_SIZE = len(training_src)\n",
    "        \n",
    "        training_X_src = [pair[0] for pair in training_src]\n",
    "        testing_X_src = [pair[0] for pair in testing_src]\n",
    "        all_data = list(itertools.chain.from_iterable(training_X_src))\n",
    "        word_counter = Counter(all_data).most_common(self.vocab_size-2)\n",
    "        del all_data\n",
    "        \n",
    "        print('='*100)\n",
    "        print('top 10 frequent words:')\n",
    "        print(word_counter[0:10])\n",
    "        self.word2idx = {val[0]: idx+1 for idx, val in enumerate(word_counter)}\n",
    "        self.word2idx['PAD'] = 0 # padding word\n",
    "        self.word2idx['OOV'] = self.vocab_size - 1 # out-of-vocabulary\n",
    "        self.idx2word = dict(zip(self.word2idx.values(), self.word2idx.keys()))\n",
    "        print('Total vocabulary size:{}'.format(len(self.word2idx)))\n",
    "        \n",
    "        \n",
    "        self.training = self.generate_batch_data(training_src)\n",
    "        self.testing = self.generate_batch_data(testing_src)       \n",
    "        \n",
    "    \n",
    "    def generate_batch_data(self, data):\n",
    "        sorted_data = sorted(data, key=lambda x: len(x[0]))\n",
    "        num_batches = int(math.floor(len(data) / self.batch_size))\n",
    "        rtn_data = []\n",
    "        for i in range(num_batches):\n",
    "            rtn_data.append(self.pad_data(sorted_data[i*self.batch_size: (i+1)*self.batch_size]))\n",
    "            \n",
    "            '''\n",
    "            # print test data\n",
    "            if i<200 and i>190:\n",
    "                print('*'*100)\n",
    "                aaa = rtn_data[i]\n",
    "                for tmp in aaa:\n",
    "                    print('-'*10)\n",
    "                    print(tmp[0])\n",
    "                    print(', '.join(self.idx2word[x] for x in tmp[0]))\n",
    "                    print(tmp[1])\n",
    "            '''          \n",
    "        return rtn_data\n",
    "    \n",
    "    def pad_data(self, batch_data):\n",
    "        max_batch_len = max([len(tmp[0]) for tmp in batch_data])\n",
    "        rtn_batch = []\n",
    "        for tmp in batch_data:\n",
    "            tmp_sen = [self.word2idx[w] if w in self.word2idx else self.word2idx['OOV'] for w in tmp[0]]\n",
    "            rtn_batch.append(((tmp_sen + [self.word2idx['PAD']] * (max_batch_len - len(tmp_sen))), tmp[1]))\n",
    "        return rtn_batch\n",
    "    \n",
    "    def next_batch_training(self):\n",
    "        random.shuffle(self.training)\n",
    "        for i in range(len(self.training)):\n",
    "            batch_X = []\n",
    "            batch_y = []\n",
    "            for tmp in self.training[i]:\n",
    "                batch_X.append(tmp[0])\n",
    "                batch_y.append(self.one_hot_encode(tmp[1]))\n",
    "            yield np.array(batch_X, dtype=np.int32), np.array(batch_y, dtype=np.int32)\n",
    "           \n",
    "    def next_batch_testing(self):\n",
    "        random.shuffle(self.testing)\n",
    "        for i in range(len(self.testing)):\n",
    "            batch_X = []\n",
    "            batch_y = []\n",
    "            for tmp in self.testing[i]:\n",
    "                batch_X.append(tmp[0])\n",
    "                batch_y.append(self.one_hot_encode(tmp[1]))\n",
    "            yield np.array(batch_X, dtype=np.int32), np.array(batch_y, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    \"\"\"\n",
    "    main hyper-parameters\n",
    "    \"\"\"\n",
    "    NUM_LAYERS = 3\n",
    "    MAX_NORM = 5.0\n",
    "    # MAX_SEQ_LENGTH = 150 # no usage here, it is a variable, and it is determined by the training data\n",
    "    EMBED_SIZE = 128 # embedding dimensions\n",
    "    BATCH_SIZE = 64\n",
    "    VOCAB_SIZE = 300000 # vocabulary size\n",
    "    NUM_CLASSES = 6 # number of classes\n",
    "    FOLDER_PATH = 'sogou_corpus'\n",
    "    NUM_EPOCH = 7\n",
    "    KEEP_PROB = 0.8 # dropout rate for rnn cell\n",
    "    RNN_TYPE = 'LSTM' # LSTM or GRU\n",
    "    CHECKPOINTS_DIR = 'text_classification_LSTM_model_part2'\n",
    "    LOGDIR = 'text_classification_LSTM_logdir_part2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for better organizing Tensorflow model structure.\n",
    "\n",
    "From https://danijar.com/structuring-your-tensorflow-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def lazy_property(function):\n",
    "    \"\"\"\n",
    "    helper function from https://danijar.com/structuring-your-tensorflow-models\n",
    "    \"\"\"\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class `TextClassificationModel` defines main model, which includes `input_output`, `RNNs_model`, `score`, `cost` and `optimizer`. \n",
    "\n",
    "Multiple layers RNNs with `tf.nn.dynamic_rnn` is used.\n",
    "\n",
    "### `outputs, last_state = tf.nn.dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None)`\n",
    "\n",
    "- `inputs` size is `[batch_size, num_steps, embedding_size]`\n",
    "- `outputs` size is `[batch_size, num_steps, num_units]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextClassificationModel:\n",
    "    \"\"\"\n",
    "    Model class.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, is_training=True):\n",
    "        self.num_units = args.EMBED_SIZE\n",
    "        self.num_layers = args.NUM_LAYERS\n",
    "        self.batch_size = args.BATCH_SIZE\n",
    "        self.rnn_type = args.RNN_TYPE\n",
    "        self.is_training = is_training\n",
    "        self.clip_norm = args.MAX_NORM\n",
    "        self.keep_prob = args.KEEP_PROB\n",
    "        \n",
    "        if self.is_training:\n",
    "            self.batch_size = args.BATCH_SIZE\n",
    "        else:\n",
    "            self.batch_size = args.TESTING_SIZE\n",
    "            \n",
    "        self.num_classes = args.NUM_CLASSES\n",
    "        self.vocab_size = args.VOCAB_SIZE\n",
    "        self.best_accuracy = tf.Variable(initial_value=0.0, dtype=tf.float32, trainable=False, name='best_accuracy')\n",
    "        self.global_step = tf.Variable(initial_value=0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "        self.input_output\n",
    "        self.model\n",
    "        self.score\n",
    "        self.cost\n",
    "        self.optimizer\n",
    "        \n",
    "    @lazy_property\n",
    "    def input_output(self):\n",
    "        with tf.name_scope('input_output'):\n",
    "            input_X = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, None], name='input_X')\n",
    "            #input_X_len = tf.placeholder(dtype=tf.int32, shape=[self.batch_size], name='input_X_len')\n",
    "            output_y = tf.placeholder(dtype=tf.int32, shape = [self.batch_size, self.num_classes], name='output_y')\n",
    "        return (input_X, output_y)\n",
    "                \n",
    "        \n",
    "    @lazy_property\n",
    "    def model(self):        \n",
    "        with tf.name_scope('RNNs_model'):\n",
    "            with tf.variable_scope('embedding'):\n",
    "                with tf.device('/cpu:0'):\n",
    "                    embedding_matrix = tf.get_variable(name='embedding_matrix', shape=[self.vocab_size, self.num_units])\n",
    "                    # inputs shape: (self.batch_size, self.num_steps, self.num_units)\n",
    "                    inputs = tf.nn.embedding_lookup(params=embedding_matrix, ids=self.input_output[0], name='embed')\n",
    "\n",
    "            if self.rnn_type == 'GRU':\n",
    "                cell = tf.contrib.rnn.GRUCell(num_units=self.num_units)\n",
    "            elif self.rnn_type == 'LSTM':\n",
    "                cell = tf.contrib.rnn.BasicLSTMCell(num_units=self.num_units)\n",
    "            else:\n",
    "                raise ValueError('The input rnn type is undefined.')\n",
    "                \n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
    "            '''\n",
    "            `[cell] * num_layers` is good for avoiding variable sharing than \n",
    "            ```\n",
    "            for _ in range.num_layers:\n",
    "                cells.append(cell)\n",
    "            ```\n",
    "            '''\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([cell] * self.num_layers)            \n",
    "            \n",
    "            initial_state = cell.zero_state(batch_size=self.batch_size, dtype=tf.float32)           \n",
    "            \n",
    "            print('='*100)           \n",
    "            print('dynamic_rnn inputs type:', type(inputs)) # Tensor\n",
    "            print('dynamic_rnn inputs shape:', inputs.get_shape()) # [self.batch_size, self.num_steps, self.num_units]\n",
    "            print('='*100)            \n",
    "            \n",
    "             \n",
    "            # calculate the acutal sequence length for each batch\n",
    "            actual_batch_len = tf.cast(tf.reduce_sum(tf.sign(self.input_output[0]), 1), tf.int32)\n",
    "            \n",
    "            outputs, last_state = tf.nn.dynamic_rnn(cell, inputs, sequence_length=actual_batch_len, initial_state=initial_state)\n",
    "            \n",
    "            print('dynamic_rnn output type:', type(outputs)) # Tensor\n",
    "            print('dynamic_rnn output shape:', outputs.get_shape()) # [self.batch_size, self.num_steps, self.num_units]         \n",
    "            print('dynamic_rnn last_state type:', type(last_state)) # tuple(LSTMStateTuple or Tensor)\n",
    "            print('last_state:', last_state)\n",
    "            print('='*100)\n",
    "            \n",
    "            # method 1: obtain the last_output for the last word of each sequence\n",
    "            # from https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "            last_word_idx = tf.range(0, self.batch_size) * tf.shape(outputs)[1] + (actual_batch_len - 1)\n",
    "            last_output = tf.gather(tf.reshape(outputs, [-1, self.num_units]), last_word_idx)\n",
    "            \n",
    "            # method 2: obtain the last_output for the last word of each sequence\n",
    "            batch_range = tf.range(tf.shape(outputs)[0])\n",
    "            indices = tf.stack([batch_range, actual_batch_len - 1], axis=1)\n",
    "            last_output_2 = tf.gather_nd(outputs, indices)\n",
    "            \n",
    "            # they are TRUE\n",
    "            self.check_euqal = tf.equal(last_output, last_output_2)\n",
    "            \n",
    "            \n",
    "            print('='*100)\n",
    "            print('dynamic_rnn last_output type:', type(last_output)) # Tensor\n",
    "            print('dynamic_rnn last_output shape:', last_output.get_shape()) # [self.batch_size, self.num_units]         \n",
    "            \n",
    "        return (last_output_2, last_state)\n",
    "    \n",
    "    @lazy_property\n",
    "    def score(self):\n",
    "        \n",
    "        with tf.variable_scope('score'):\n",
    "            softmax_weights = tf.get_variable(name='softmax_weights', shape=[self.num_units, self.num_classes])\n",
    "            softmax_bias = tf.get_variable(name='softmax_bias', shape=[self.num_classes])        \n",
    "            logits = tf.matmul(self.model[0], softmax_weights) + softmax_bias\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            prediction = tf.argmax(probs, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(prediction, tf.int32), tf.cast(tf.argmax(self.input_output[1], 1), tf.int32)), tf.float32))\n",
    "            tf.summary.scalar(name='accuracy', tensor=accuracy)\n",
    "            \n",
    "        return (logits, accuracy)\n",
    "    \n",
    "    @lazy_property\n",
    "    def cost(self):            \n",
    "        with tf.name_scope('cost'):\n",
    "            cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.score[0], labels=self.input_output[1]))\n",
    "            cost_check = tf.check_numerics(cost, 'Cost is Nan')\n",
    "            with tf.control_dependencies([cost_check]):\n",
    "                tf.summary.scalar(name='loss', tensor=cost)\n",
    "                tf.summary.histogram(name='histogram_loss', values=cost)\n",
    "                self.summary_op = tf.summary.merge_all()\n",
    "        return cost\n",
    "    \n",
    "    @lazy_property\n",
    "    def optimizer(self):\n",
    "        with tf.name_scope('optimizer'):\n",
    "            variables = tf.trainable_variables()\n",
    "            gradients = tf.gradients(ys=self.cost, xs=variables)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm=self.clip_norm)\n",
    "            \n",
    "            #grad_check = tf.check_numerics(clipped_gradients, 'Gradients is Nan')\n",
    "            #with tf.control_dependencies([grad_check]):\n",
    "            #starter_learning_rate = 0.05\n",
    "            #decay_rate = tf.train.exponential_decay(starter_learning_rate, self.global_step, 1000, 0.96)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "            optimize = optimizer.apply_gradients(grads_and_vars=zip(clipped_gradients, variables), global_step=self.global_step)\n",
    "            return optimize    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data, model, args):\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        train_writer = tf.summary.FileWriter(logdir=args.LOGDIR + '/train', graph=sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(logdir=args.LOGDIR + '/test', graph=sess.graph)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir=args.CHECKPOINTS_DIR)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess=sess, save_path=ckpt.model_checkpoint_path)\n",
    "            print(ckpt)\n",
    "        \n",
    "        initial_step = model.global_step.eval()\n",
    "        idx = 1\n",
    "        for loop in range(args.NUM_EPOCH):\n",
    "            for batch_X, batch_y in data.next_batch_training():\n",
    "\n",
    "                feed_dict = {model.input_output: (batch_X, batch_y)}\n",
    "                aaa, tmp_accuracy, tmp_cost, _, tmp_summary = sess.run([model.check_euqal, model.score[1], model.cost, model.optimizer, model.summary_op], feed_dict=feed_dict)\n",
    "                train_writer.add_summary(summary=tmp_summary, global_step=model.global_step.eval())\n",
    "               \n",
    "                \n",
    "                if idx % 50 == 0:\n",
    "                    #print(aaa)\n",
    "                    print('='*100)\n",
    "                    print('Step:{}, training accuracy:{:4f}'.format(model.global_step.eval(), tmp_accuracy))\n",
    "                    print('loop / idx: {} / {}, loss:{:4f}, accuracy:{:4f}'.format(loop, idx, tmp_cost, tmp_accuracy))\n",
    "                    print('='*100)\n",
    "\n",
    "                if idx % 1000 == 0:\n",
    "                    test_cost = 0.0\n",
    "                    test_accuracy = 0.0\n",
    "                    cc = 0\n",
    "                    for test_batch_X, test_batch_y in data.next_batch_testing():\n",
    "                        test_feed_dict = {model.input_output:(test_batch_X, test_batch_y)}\n",
    "                        test_tmp_cost, test_tmp_accuracy, test_tmp_summary = sess.run([model.cost, model.score[1], model.summary_op], feed_dict=test_feed_dict)\n",
    "                        test_writer.add_summary(summary=test_tmp_summary, global_step=model.global_step.eval())\n",
    "                        test_cost += test_tmp_cost\n",
    "                        test_accuracy += test_tmp_accuracy\n",
    "                        cc += 1                    \n",
    "                    print('-'*100)\n",
    "                    test_accuracy = test_accuracy / cc\n",
    "                    test_cost = test_cost / cc\n",
    "                    print('Step:{}, average testing cost:{:4f}, average testing accuracy:{:4f}'.format(model.global_step.eval(), test_cost, test_accuracy))\n",
    "                    print('-'*100)\n",
    "                    \n",
    "                    if test_accuracy >= sess.run(model.best_accuracy):\n",
    "                        print('Best model accuracy: {:4f}'.format(test_accuracy))\n",
    "                        sess.run(model.best_accuracy.assign(test_accuracy))\n",
    "                        saver.save(sess=sess, save_path=os.path.join(args.CHECKPOINTS_DIR, 'text_classification_lstm.ckpt'), global_step=model.global_step.eval())\n",
    "                                        \n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing file >>>>>>>>>>>>>>>  1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "parsing file >>>>>>>>>>>>>>>  2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "parsing file >>>>>>>>>>>>>>>  3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "parsing file >>>>>>>>>>>>>>>  4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "parsing file >>>>>>>>>>>>>>>  5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "parsing file >>>>>>>>>>>>>>>  6\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "Size of training data: 90000\n",
      "Size of testing data: 18000\n",
      "====================================================================================================\n",
      "top 10 frequent words:\n",
      "[('系列', 88969), ('月', 77879), ('中', 70570), ('年', 64838), ('产品', 61786), ('日', 58050), ('英寸', 57891), ('华硕', 56191), ('屏幕尺寸', 53018), ('主频', 52638)]\n",
      "Total vocabulary size:300000\n",
      "====================================================================================================\n",
      "dynamic_rnn inputs type: <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "dynamic_rnn inputs shape: (64, ?, 128)\n",
      "====================================================================================================\n",
      "dynamic_rnn output type: <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "dynamic_rnn output shape: (64, ?, 128)\n",
      "dynamic_rnn last_state type: <class 'tuple'>\n",
      "last_state: (LSTMStateTuple(c=<tf.Tensor 'RNNs_model/rnn/while/Exit_2:0' shape=(64, 128) dtype=float32>, h=<tf.Tensor 'RNNs_model/rnn/while/Exit_3:0' shape=(64, 128) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'RNNs_model/rnn/while/Exit_4:0' shape=(64, 128) dtype=float32>, h=<tf.Tensor 'RNNs_model/rnn/while/Exit_5:0' shape=(64, 128) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'RNNs_model/rnn/while/Exit_6:0' shape=(64, 128) dtype=float32>, h=<tf.Tensor 'RNNs_model/rnn/while/Exit_7:0' shape=(64, 128) dtype=float32>))\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "dynamic_rnn last_output type: <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "dynamic_rnn last_output shape: (64, 128)\n",
      "====================================================================================================\n",
      "Step:50, training accuracy:0.000000\n",
      "loop / idx: 0 / 50, loss:2.932506, accuracy:0.000000\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:100, training accuracy:0.062500\n",
      "loop / idx: 0 / 100, loss:1.928157, accuracy:0.062500\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:150, training accuracy:0.046875\n",
      "loop / idx: 0 / 150, loss:1.911428, accuracy:0.046875\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:200, training accuracy:0.000000\n",
      "loop / idx: 0 / 200, loss:1.849182, accuracy:0.000000\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:250, training accuracy:0.312500\n",
      "loop / idx: 0 / 250, loss:1.671263, accuracy:0.312500\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:300, training accuracy:0.218750\n",
      "loop / idx: 0 / 300, loss:1.679508, accuracy:0.218750\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:350, training accuracy:0.609375\n",
      "loop / idx: 0 / 350, loss:1.486752, accuracy:0.609375\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:400, training accuracy:0.171875\n",
      "loop / idx: 0 / 400, loss:1.687066, accuracy:0.171875\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:450, training accuracy:0.562500\n",
      "loop / idx: 0 / 450, loss:1.291369, accuracy:0.562500\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:500, training accuracy:0.656250\n",
      "loop / idx: 0 / 500, loss:0.744096, accuracy:0.656250\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:550, training accuracy:0.718750\n",
      "loop / idx: 0 / 550, loss:0.753791, accuracy:0.718750\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:600, training accuracy:0.843750\n",
      "loop / idx: 0 / 600, loss:0.639126, accuracy:0.843750\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:650, training accuracy:0.578125\n",
      "loop / idx: 0 / 650, loss:0.979218, accuracy:0.578125\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:700, training accuracy:0.953125\n",
      "loop / idx: 0 / 700, loss:0.193906, accuracy:0.953125\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:750, training accuracy:0.937500\n",
      "loop / idx: 0 / 750, loss:0.264445, accuracy:0.937500\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:800, training accuracy:0.906250\n",
      "loop / idx: 0 / 800, loss:0.287068, accuracy:0.906250\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:850, training accuracy:0.921875\n",
      "loop / idx: 0 / 850, loss:0.225682, accuracy:0.921875\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:900, training accuracy:0.812500\n",
      "loop / idx: 0 / 900, loss:0.573016, accuracy:0.812500\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:950, training accuracy:0.937500\n",
      "loop / idx: 0 / 950, loss:0.329069, accuracy:0.937500\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1000, training accuracy:0.984375\n",
      "loop / idx: 0 / 1000, loss:0.073627, accuracy:0.984375\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Step:1000, average testing cost:0.375240, average testing accuracy:0.897409\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Best model accuracy: 0.897409\n",
      "====================================================================================================\n",
      "Step:1050, training accuracy:0.921875\n",
      "loop / idx: 0 / 1050, loss:0.147111, accuracy:0.921875\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1100, training accuracy:0.750000\n",
      "loop / idx: 0 / 1100, loss:0.786429, accuracy:0.750000\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1150, training accuracy:0.906250\n",
      "loop / idx: 0 / 1150, loss:0.379448, accuracy:0.906250\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1200, training accuracy:0.796875\n",
      "loop / idx: 0 / 1200, loss:0.600435, accuracy:0.796875\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1250, training accuracy:0.937500\n",
      "loop / idx: 0 / 1250, loss:0.328683, accuracy:0.937500\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1300, training accuracy:0.953125\n",
      "loop / idx: 0 / 1300, loss:0.213153, accuracy:0.953125\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1350, training accuracy:0.968750\n",
      "loop / idx: 0 / 1350, loss:0.196135, accuracy:0.968750\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1400, training accuracy:0.953125\n",
      "loop / idx: 0 / 1400, loss:0.220879, accuracy:0.953125\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1450, training accuracy:0.968750\n",
      "loop / idx: 1 / 1450, loss:0.107970, accuracy:0.968750\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1500, training accuracy:0.937500\n",
      "loop / idx: 1 / 1500, loss:0.181534, accuracy:0.937500\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1550, training accuracy:0.875000\n",
      "loop / idx: 1 / 1550, loss:0.494121, accuracy:0.875000\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1600, training accuracy:0.890625\n",
      "loop / idx: 1 / 1600, loss:0.354091, accuracy:0.890625\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1650, training accuracy:0.890625\n",
      "loop / idx: 1 / 1650, loss:0.426245, accuracy:0.890625\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1700, training accuracy:1.000000\n",
      "loop / idx: 1 / 1700, loss:0.010517, accuracy:1.000000\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1750, training accuracy:0.906250\n",
      "loop / idx: 1 / 1750, loss:0.142283, accuracy:0.906250\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1800, training accuracy:0.984375\n",
      "loop / idx: 1 / 1800, loss:0.067941, accuracy:0.984375\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1850, training accuracy:0.968750\n",
      "loop / idx: 1 / 1850, loss:0.158102, accuracy:0.968750\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1900, training accuracy:0.984375\n",
      "loop / idx: 1 / 1900, loss:0.087071, accuracy:0.984375\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:1950, training accuracy:0.859375\n",
      "loop / idx: 1 / 1950, loss:0.410031, accuracy:0.859375\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2000, training accuracy:0.984375\n",
      "loop / idx: 1 / 2000, loss:0.106901, accuracy:0.984375\n",
      "====================================================================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step:2000, average testing cost:0.197329, average testing accuracy:0.939724\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Best model accuracy: 0.939724\n",
      "====================================================================================================\n",
      "Step:2050, training accuracy:0.890625\n",
      "loop / idx: 1 / 2050, loss:0.198570, accuracy:0.890625\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2100, training accuracy:0.968750\n",
      "loop / idx: 1 / 2100, loss:0.142389, accuracy:0.968750\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2150, training accuracy:0.968750\n",
      "loop / idx: 1 / 2150, loss:0.077163, accuracy:0.968750\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2200, training accuracy:0.953125\n",
      "loop / idx: 1 / 2200, loss:0.100501, accuracy:0.953125\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2250, training accuracy:0.953125\n",
      "loop / idx: 1 / 2250, loss:0.095879, accuracy:0.953125\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2300, training accuracy:0.968750\n",
      "loop / idx: 1 / 2300, loss:0.109144, accuracy:0.968750\n",
      "====================================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================================================================\n",
      "Step:2350, training accuracy:0.984375\n",
      "loop / idx: 1 / 2350, loss:0.071001, accuracy:0.984375\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2400, training accuracy:1.000000\n",
      "loop / idx: 1 / 2400, loss:0.001901, accuracy:1.000000\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2450, training accuracy:0.968750\n",
      "loop / idx: 1 / 2450, loss:0.113970, accuracy:0.968750\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2500, training accuracy:0.984375\n",
      "loop / idx: 1 / 2500, loss:0.062683, accuracy:0.984375\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2550, training accuracy:0.953125\n",
      "loop / idx: 1 / 2550, loss:0.162526, accuracy:0.953125\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2600, training accuracy:0.953125\n",
      "loop / idx: 1 / 2600, loss:0.170751, accuracy:0.953125\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2650, training accuracy:0.953125\n",
      "loop / idx: 1 / 2650, loss:0.118053, accuracy:0.953125\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2700, training accuracy:0.968750\n",
      "loop / idx: 1 / 2700, loss:0.133201, accuracy:0.968750\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2750, training accuracy:0.968750\n",
      "loop / idx: 1 / 2750, loss:0.230817, accuracy:0.968750\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2800, training accuracy:0.984375\n",
      "loop / idx: 1 / 2800, loss:0.038225, accuracy:0.984375\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2850, training accuracy:0.953125\n",
      "loop / idx: 2 / 2850, loss:0.132338, accuracy:0.953125\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2900, training accuracy:0.968750\n",
      "loop / idx: 2 / 2900, loss:0.199780, accuracy:0.968750\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:2950, training accuracy:1.000000\n",
      "loop / idx: 2 / 2950, loss:0.035863, accuracy:1.000000\n",
      "====================================================================================================\n",
      "====================================================================================================\n",
      "Step:3000, training accuracy:0.984375\n",
      "loop / idx: 2 / 3000, loss:0.073269, accuracy:0.984375\n",
      "====================================================================================================\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Step:3000, average testing cost:0.213703, average testing accuracy:0.937834\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b42a00588104>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextClassificationModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-c99efa15417c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data, model, args)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_output\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                 \u001b[0maaa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_euqal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                 \u001b[0mtrain_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtmp_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda_2.7\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda_2.7\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda_2.7\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda_2.7\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda_2.7\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = Arguments()\n",
    "    data = DataGenerator(args)\n",
    "\n",
    "    model = TextClassificationModel(args)\n",
    "    train(data, model, args)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
