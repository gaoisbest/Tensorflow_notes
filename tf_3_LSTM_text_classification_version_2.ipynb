{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is part 2 of text classification based on Recurrent Neural Networks (RNNs).\n",
    "\n",
    "### Core concepts in part 2:\n",
    "- **Multiple RNNs layers** with **`tf.nn.dynamic_rnn`**, which just copy the cell state through and output zero-tensor after real sequence length.\n",
    "- **Variable sequence length**\n",
    "    - For model input, to achieve minimal padding, I first sort the data, then pad them to same sequence length in each batch. Note:**The less the padding, the fast the training is**.\n",
    "    - For model output, the hidden state at the actual sequence length step should be extracted.\n",
    "- **Exploding and vanishing gradients**\n",
    "\n",
    "### Variable sequence length\n",
    "- **Input**:\n",
    "    - Passing actual sequence lengths to `sequence_length` of `tf.nn.dynamic_rnn` via `actual_seq_len = tf.cast(tf.reduce_sum(tf.sign(self.input_X), 1), tf.int32)` [1].\n",
    "- **Output**:\n",
    "    - We should extract the hidden state at last actual step. This can be down by the following codes [1]:\n",
    "    ```\n",
    "    def last_relevant(rnn_output, seq_length):\n",
    "        batch_size = tf.shape(output)[0]\n",
    "        max_length = tf.shape(output)[1]\n",
    "        out_size = int(rnn_output.get_shape()[2])\n",
    "        index = tf.range(0, batch_size) * max_length + (seq_length - 1)\n",
    "        flat = tf.reshape(rnn_output, [-1, out_size])\n",
    "        relevant = tf.gather(flat, index)\n",
    "        return relevant\n",
    "    ```\n",
    "- **Trick**\n",
    "    - **Bucketing** can be used to **accelerate** the training process, but do not have the effect on the model accuracy. Each bucket has a corresponding `placeholder`, which corresponds to a sub-graph, and they share other parts of whole computation graph [2]. Not implemented yet. \n",
    "\n",
    "### Exploding and vanishing gradients\n",
    "- **Vanishing gradients**\n",
    "    - Appearance: the weights of last layers change a lot more than those at the beginning layers.\n",
    "    - Why [LSTM / GRU prevents vanishing gradients](https://www.quora.com/How-does-LSTM-help-prevent-the-vanishing-and-exploding-gradient-problem-in-a-recurrent-neural-network) ? \n",
    "\n",
    "- **Exploding gradients**\n",
    "    - `tf.clip_by_global_norm` for avoiding exploding gradients.\n",
    "    ```\n",
    "    variables = tf.trainable_variables()\n",
    "    gradients = tf.gradients(ys=self.cost, xs=variables)\n",
    "    clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm=self.clip_norm)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "    optimize = optimizer.apply_gradients(grads_and_vars=zip(clipped_gradients, variables), global_step=self.global_step)\n",
    "    ```\n",
    "\n",
    "\n",
    "References:  \n",
    "[1] https://danijar.com/variable-sequence-lengths-in-tensorflow/  \n",
    "[2] https://r2rt.com/recurrent-neural-networks-in-tensorflow-iii-variable-length-sequences.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import codecs\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from random import shuffle\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class `DataGenerator` is used to read input files, convert words to index and generate batch training or testing data. \n",
    "\n",
    "Two extra word are introduced, `PAD` for padding shorter sequences and `OOV` for representing out-of-vocabulary words.\n",
    "\n",
    "Sorting data then pad them in each batch to achieve minimal padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    \"\"\"\n",
    "    reading each training and testing files, and generating batch data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        self.folder_path = args.FOLDER_PATH\n",
    "        self.batch_size = args.BATCH_SIZE\n",
    "        self.vocab_size = args.VOCAB_SIZE\n",
    "        self.num_epoch = args.NUM_EPOCH\n",
    "        self.read_build_input()\n",
    "        self.label_dict = {0:'auto', 1:'business', 2:'IT', 3:'health', 4:'sports', 5:'yule'}\n",
    "        \n",
    "        \n",
    "    def read_build_input(self):\n",
    "        training_src = []\n",
    "        testing_src = []\n",
    "        training_article_len = []\n",
    "\n",
    "        for cur_category in range(1, 7):\n",
    "            \n",
    "            print('parsing file >>>>>>>>>>>>>>> ', cur_category)\n",
    "            print('-'*100)\n",
    "            \n",
    "            training_input_file = codecs.open(filename=os.path.join(self.folder_path, 'training_' + str(cur_category) + '.cs'), mode='r', encoding='utf-8')\n",
    "            for tmp_line in training_input_file:\n",
    "                training_src.append((tmp_line.split(), cur_category-1))\n",
    "                training_article_len.append(len(tmp_line.split()))\n",
    "\n",
    "            testing_input_file = codecs.open(filename=os.path.join(self.folder_path, 'testing_' + str(cur_category) + '.cs'), mode='r', encoding='utf-8')\n",
    "            for tmp_line in testing_input_file:\n",
    "                testing_src.append((tmp_line.split(), cur_category-1))\n",
    "\n",
    "        print('='*100)\n",
    "        print('Size of training data:', len(training_src))\n",
    "        print('Size of testing data:', len(testing_src))\n",
    "            \n",
    "        self.TRAINING_SIZE = len(training_src)\n",
    "        \n",
    "        training_X_src = [pair[0] for pair in training_src]\n",
    "        testing_X_src = [pair[0] for pair in testing_src]\n",
    "        all_data = list(itertools.chain.from_iterable(training_X_src))\n",
    "        word_counter = Counter(all_data).most_common(self.vocab_size-2)\n",
    "        del all_data\n",
    "        \n",
    "        print('='*100)\n",
    "        print('top 10 frequent words:')\n",
    "        print(word_counter[0:10])\n",
    "        self.word2idx = {val[0]: idx+1 for idx, val in enumerate(word_counter)}\n",
    "        self.word2idx['PAD'] = 0 # padding word\n",
    "        self.word2idx['OOV'] = self.vocab_size - 1 # out-of-vocabulary\n",
    "        self.idx2word = dict(zip(self.word2idx.values(), self.word2idx.keys()))\n",
    "        print('Total vocabulary size:{}'.format(len(self.word2idx)))\n",
    "        \n",
    "        \n",
    "        self.training = self.generate_batch_data(training_src)\n",
    "        self.testing = self.generate_batch_data(testing_src)       \n",
    "        \n",
    "    \n",
    "    def generate_batch_data(self, data):\n",
    "        sorted_data = sorted(data, key=lambda x: len(x[0]))\n",
    "        num_batches = int(math.floor(len(data) / self.batch_size))\n",
    "        rtn_data = []\n",
    "        for i in range(num_batches):\n",
    "            rtn_data.append(self.pad_data(sorted_data[i*self.batch_size: (i+1)*self.batch_size]))\n",
    "            \n",
    "            '''\n",
    "            # print test data\n",
    "            if i<200 and i>190:\n",
    "                print('*'*100)\n",
    "                aaa = rtn_data[i]\n",
    "                for tmp in aaa:\n",
    "                    print('-'*10)\n",
    "                    print(tmp[0])\n",
    "                    print(', '.join(self.idx2word[x] for x in tmp[0]))\n",
    "                    print(tmp[1])\n",
    "            '''          \n",
    "        return rtn_data\n",
    "    \n",
    "    def pad_data(self, batch_data):\n",
    "        max_batch_len = max([len(tmp[0]) for tmp in batch_data])\n",
    "        rtn_batch = []\n",
    "        for tmp in batch_data:\n",
    "            tmp_sen = [self.word2idx[w] if w in self.word2idx else self.word2idx['OOV'] for w in tmp[0]]\n",
    "            rtn_batch.append(((tmp_sen + [self.word2idx['PAD']] * (max_batch_len - len(tmp_sen))), tmp[1]))\n",
    "        return rtn_batch\n",
    "    \n",
    "    def next_batch_training(self):\n",
    "        random.shuffle(self.training)\n",
    "        for i in range(len(self.training)):\n",
    "            batch_X = []\n",
    "            batch_y = []\n",
    "            for tmp in self.training[i]:\n",
    "                batch_X.append(tmp[0])\n",
    "                batch_y.append(tmp[1])\n",
    "            yield np.array(batch_X, dtype=np.int32), np.array(batch_y, dtype=np.int32)\n",
    "           \n",
    "    def next_batch_testing(self):\n",
    "        random.shuffle(self.testing)\n",
    "        for i in range(len(self.testing)):\n",
    "            batch_X = []\n",
    "            batch_y = []\n",
    "            for tmp in self.testing[i]:\n",
    "                batch_X.append(tmp[0])\n",
    "                batch_y.append(tmp[1])\n",
    "            yield np.array(batch_X, dtype=np.int32), np.array(batch_y, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Arguments:\n",
    "    \"\"\"\n",
    "    main hyper-parameters\n",
    "    \"\"\"\n",
    "    NUM_LAYERS = 3\n",
    "    MAX_NORM = 5.0\n",
    "    # MAX_SEQ_LENGTH = 150 # no usage here, it is a variable, and it is determined by the training data\n",
    "    EMBED_SIZE = 128 # embedding dimensions\n",
    "    BATCH_SIZE = 64\n",
    "    VOCAB_SIZE = 300000 # vocabulary size\n",
    "    NUM_CLASSES = 6 # number of classes\n",
    "    FOLDER_PATH = 'sogou_corpus'\n",
    "    NUM_EPOCH = 7\n",
    "    KEEP_PROB = 0.8 # dropout rate for rnn cell\n",
    "    RNN_TYPE = 'LSTM' # LSTM or GRU\n",
    "    CHECKPOINTS_DIR = 'text_classification_LSTM_model'\n",
    "    LOGDIR = 'text_classification_LSTM_logdir'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for better organizing Tensorflow model structure.\n",
    "\n",
    "From https://danijar.com/structuring-your-tensorflow-models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "def lazy_property(function):\n",
    "    \"\"\"\n",
    "    helper function from https://danijar.com/structuring-your-tensorflow-models\n",
    "    \"\"\"\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class `TextClassificationModel` defines main model, which includes `input_output`, `RNNs_model`, `score`, `cost` and `optimizer`. \n",
    "\n",
    "Multiple layers RNNs with `tf.nn.dynamic_rnn` is used.\n",
    "\n",
    "### `outputs, last_state = tf.nn.dynamic_rnn(cell, inputs, sequence_length=None, initial_state=None)`\n",
    "\n",
    "- `inputs` and `outputs` are `Tensor`, both size are `[batch_size, num_steps, num_units]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TextClassificationModel:\n",
    "    \"\"\"\n",
    "    Model class.\n",
    "    \"\"\"\n",
    "    def __init__(self, args, is_training=True):\n",
    "        self.num_units = args.EMBED_SIZE\n",
    "        self.num_layers = args.NUM_LAYERS\n",
    "        self.batch_size = args.BATCH_SIZE\n",
    "        self.rnn_type = args.RNN_TYPE\n",
    "        self.is_training = is_training\n",
    "        self.clip_norm = args.MAX_NORM\n",
    "        self.keep_prob = args.KEEP_PROB\n",
    "        \n",
    "        if self.is_training:\n",
    "            self.batch_size = args.BATCH_SIZE\n",
    "        else:\n",
    "            self.batch_size = args.TESTING_SIZE\n",
    "            \n",
    "        self.num_classes = args.NUM_CLASSES\n",
    "        self.vocab_size = args.VOCAB_SIZE\n",
    "        self.best_accuracy = tf.Variable(initial_value=0.0, dtype=tf.float32, trainable=False, name='best_accuracy')\n",
    "        self.global_step = tf.Variable(initial_value=0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "        self.input_output\n",
    "        self.model\n",
    "        self.score\n",
    "        self.cost\n",
    "        self.optimizer\n",
    "        \n",
    "    @lazy_property\n",
    "    def input_output(self):\n",
    "        with tf.name_scope('input_output'):\n",
    "            input_X = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, None], name='input_X')\n",
    "            #input_X_len = tf.placeholder(dtype=tf.int32, shape=[self.batch_size], name='input_X_len')\n",
    "            output_y = tf.placeholder(dtype=tf.int32, shape = [self.batch_size], name='output_y')\n",
    "        return (input_X, output_y)\n",
    "                \n",
    "        \n",
    "    @lazy_property\n",
    "    def model(self):        \n",
    "        with tf.name_scope('RNNs_model'):\n",
    "            with tf.variable_scope('embedding'):\n",
    "                with tf.device('/cpu:0'):\n",
    "                    embedding_matrix = tf.get_variable(name='embedding_matrix', shape=[self.vocab_size, self.num_units])\n",
    "                    # inputs shape: (self.batch_size, self.num_steps, self.num_units)\n",
    "                    inputs = tf.nn.embedding_lookup(params=embedding_matrix, ids=self.input_output[0], name='embed')\n",
    "\n",
    "            if self.rnn_type == 'GRU':\n",
    "                cell = tf.contrib.rnn.GRUCell(num_units=self.num_units)\n",
    "            elif self.rnn_type == 'LSTM':\n",
    "                cell = tf.contrib.rnn.BasicLSTMCell(num_units=self.num_units)\n",
    "            else:\n",
    "                raise ValueError('The input rnn type is undefined.')\n",
    "                \n",
    "            cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=self.keep_prob)\n",
    "            '''\n",
    "            `[cell] * num_layers` is good for avoiding variable sharing than \n",
    "            ```\n",
    "            for _ in range.num_layers:\n",
    "                cells.append(cell)\n",
    "            ```\n",
    "            '''\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([cell] * self.num_layers)            \n",
    "            \n",
    "            initial_state = cell.zero_state(batch_size=self.batch_size, dtype=tf.float32)           \n",
    "            \n",
    "            print('='*100)           \n",
    "            print('dynamic_rnn inputs type:', type(inputs)) # Tensor\n",
    "            print('dynamic_rnn inputs shape:', inputs.get_shape()) # [self.batch_size, self.num_steps, self.num_units]\n",
    "            print('='*100)            \n",
    "            \n",
    "             \n",
    "            # calculate the acutal sequence length for each batch\n",
    "            actual_batch_len = tf.cast(tf.reduce_sum(tf.sign(self.input_output[0]), 1), tf.int32)\n",
    "            \n",
    "            outputs, last_state = tf.nn.dynamic_rnn(cell, inputs, sequence_length=actual_batch_len, initial_state=initial_state)\n",
    "            \n",
    "            print('dynamic_rnn output type:', type(outputs)) # Tensor\n",
    "            print('dynamic_rnn output shape:', outputs.get_shape()) # [self.batch_size, self.num_steps, self.num_units]         \n",
    "            print('dynamic_rnn last_state type:', type(last_state)) # tuple(LSTMStateTuple or Tensor)\n",
    "            print('last_state:', last_state)\n",
    "            print('='*100)\n",
    "            \n",
    "            # obtain the last_output for the last word of each sequence\n",
    "            # from https://danijar.com/variable-sequence-lengths-in-tensorflow/\n",
    "            last_word_idx = tf.range(0, self.batch_size) * tf.shape(outputs)[1] + (actual_batch_len - 1)\n",
    "            last_output = tf.gather(tf.reshape(outputs, [-1, self.num_units]), last_word_idx)\n",
    "            \n",
    "            \n",
    "            batch_range = tf.range(tf.shape(outputs)[0])\n",
    "            indices = tf.stack([batch_range, actual_batch_len - 1], axis=1)\n",
    "            last_output_2 = tf.gather_nd(outputs, indices)\n",
    "            print('tf.shape(outputs):', tf.shape(outputs))\n",
    "            print('batch_range:', batch_range)\n",
    "            print('indices', indices)\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #print('last_output == last_output_2', tf.equal(last_output, last_output_2))\n",
    "            self.check_euqal = tf.equal(last_output, last_output_2)\n",
    "            \n",
    "            \n",
    "            print('='*100)\n",
    "            print('dynamic_rnn last_output type:', type(last_output)) # Tensor\n",
    "            print('dynamic_rnn last_output shape:', last_output.get_shape()) # [self.batch_size, self.num_units]         \n",
    "            \n",
    "            '''\n",
    "            # get the output of last time step directly\n",
    "            # using outputs[:, -1, :] or tf.gather\n",
    "            # see https://danijar.com/introduction-to-recurrent-networks-in-tensorflow\n",
    "            output = tf.transpose(outputs, [1, 0, 2])\n",
    "            last = tf.gather(output, int(output.get_shape()[0]) - 1)            \n",
    "            tf.equal(last, outputs[:, -1, :]) is True            \n",
    "            '''\n",
    "\n",
    "        return (last_output, last_state)\n",
    "    \n",
    "    @lazy_property\n",
    "    def score(self):\n",
    "        \n",
    "        with tf.variable_scope('score'):\n",
    "            softmax_weights = tf.get_variable(name='softmax_weights', shape=[self.num_units, self.num_classes])\n",
    "            softmax_bias = tf.get_variable(name='softmax_bias', shape=[self.num_classes])        \n",
    "            logits = tf.matmul(self.model[0], softmax_weights) + softmax_bias\n",
    "            probs = tf.nn.softmax(logits)\n",
    "            prediction = tf.argmax(probs, 1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(prediction, tf.int32), self.input_output[1]), tf.float32))\n",
    "            tf.summary.scalar(name='accuracy', tensor=accuracy)\n",
    "            \n",
    "        return (logits, accuracy)\n",
    "    \n",
    "    @lazy_property\n",
    "    def cost(self):            \n",
    "        with tf.name_scope('cost'):\n",
    "            cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.score[0], labels=self.input_output[1]))\n",
    "            tf.summary.scalar(name='loss', tensor=cost)\n",
    "            tf.summary.histogram(name='histogram_loss', values=cost)\n",
    "            self.summary_op = tf.summary.merge_all()\n",
    "        return cost\n",
    "    \n",
    "    @lazy_property\n",
    "    def optimizer(self):\n",
    "        with tf.name_scope('optimizer'):\n",
    "            variables = tf.trainable_variables()\n",
    "            gradients = tf.gradients(ys=self.cost, xs=variables)\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(gradients, clip_norm=self.clip_norm)\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "            optimize = optimizer.apply_gradients(grads_and_vars=zip(clipped_gradients, variables), global_step=self.global_step)\n",
    "            return optimize\n",
    "                          \n",
    "                \n",
    "    def predict(self, sess, data):\n",
    "        testing_X = np.array([tmp_pair[0] for tmp_pair in data.testing], dtype=np.int32)\n",
    "        testing_y = np.array([tmp_pair[1] for tmp_pair in data.testing], dtype=np.int32)\n",
    "        feed_dict = {self.input_X:testing_X, self.output_y:testing_y}\n",
    "        predict_labels, predict_accuracy = sess.run([self.prediction, self.accuracy], feed_dict=feed_dict)\n",
    "        print('============================Example of predictions============================')\n",
    "        for i in range(10):\n",
    "            print('-'*100)\n",
    "            print('Article: ', ''.join([data.idx2word[idx] for idx in testing_X[i]]))\n",
    "            print('Real category: ', data.label_dict[testing_y[i]])\n",
    "            print('Predicted category: ', data.label_dict[predict_labels[i]])\n",
    "            print('-'*100 + '\\n')\n",
    "        return predict_labels, predict_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(data, model, args):\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        train_writer = tf.summary.FileWriter(logdir=args.LOGDIR + '/train', graph=sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(logdir=args.LOGDIR + '/test', graph=sess.graph)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        ckpt = tf.train.get_checkpoint_state(checkpoint_dir=args.CHECKPOINTS_DIR)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess=sess, save_path=ckpt.model_checkpoint_path)\n",
    "            print(ckpt)\n",
    "        \n",
    "        initial_step = model.global_step.eval()\n",
    "        idx = 1\n",
    "        for loop in range(args.NUM_EPOCH):\n",
    "            for batch_X, batch_y in data.next_batch_training():\n",
    "\n",
    "                feed_dict = {model.input_output: (batch_X, batch_y)}\n",
    "                aaa, tmp_accuracy, tmp_cost, _, tmp_summary = sess.run([model.check_euqal, model.score[1], model.cost, model.optimizer, model.summary_op], feed_dict=feed_dict)\n",
    "                train_writer.add_summary(summary=tmp_summary, global_step=model.global_step.eval())\n",
    "               \n",
    "                \n",
    "                if idx % 50 == 0:\n",
    "                    #print(aaa)\n",
    "                    print('='*100)\n",
    "                    print('Step:{}, training accuracy:{:4f}'.format(model.global_step.eval(), tmp_accuracy))\n",
    "                    print('loop / idx: {} / {}, loss:{:4f}, accuracy:{:4f}'.format(loop, idx, tmp_cost, tmp_accuracy))\n",
    "                    print('='*100)\n",
    "\n",
    "                if idx % 1000 == 0:\n",
    "                    test_cost = 0.0\n",
    "                    test_accuracy = 0.0\n",
    "                    cc = 0\n",
    "                    for test_batch_X, test_batch_y in data.next_batch_testing():\n",
    "                        test_feed_dict = {model.input_output:(test_batch_X, test_batch_y)}\n",
    "                        test_tmp_cost, test_tmp_accuracy, test_tmp_summary = sess.run([model.cost, model.score[1], model.summary_op], feed_dict=test_feed_dict)\n",
    "                        test_writer.add_summary(summary=test_tmp_summary, global_step=model.global_step.eval())\n",
    "                        test_cost += test_tmp_cost\n",
    "                        test_accuracy += test_tmp_accuracy\n",
    "                        cc += 1                    \n",
    "                    print('-'*100)\n",
    "                    test_accuracy = test_accuracy / cc\n",
    "                    test_cost = test_cost / cc\n",
    "                    print('Step:{}, average testing cost:{:4f}, average testing accuracy:{:4f}'.format(model.global_step.eval(), test_cost, test_accuracy))\n",
    "                    print('-'*100)\n",
    "                    \n",
    "                    if test_accuracy >= sess.run(model.best_accuracy):\n",
    "                        print('Best model accuracy: {:4f}'.format(test_accuracy))\n",
    "                        sess.run(model.best_accuracy.assign(test_accuracy))\n",
    "                        saver.save(sess=sess, save_path=os.path.join(args.CHECKPOINTS_DIR, 'text_classification_lstm.ckpt'), global_step=model.global_step.eval())\n",
    "                                        \n",
    "                idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(data, model, args):\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "        ckpt = tf.train.latest_checkpoint(args.CHECKPOINTS_DIR)\n",
    "        print(ckpt)\n",
    "        saver.restore(sess=sess, save_path=ckpt)\n",
    "        predict_labels, predict_accuracy = model.predict(sess, data)\n",
    "        print('predict_accuracy:{:5f}'.format(predict_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing file >>>>>>>>>>>>>>>  1\n",
      "----------------------------------------------------------------------------------------------------\n",
      "parsing file >>>>>>>>>>>>>>>  2\n",
      "----------------------------------------------------------------------------------------------------\n",
      "parsing file >>>>>>>>>>>>>>>  3\n",
      "----------------------------------------------------------------------------------------------------\n",
      "parsing file >>>>>>>>>>>>>>>  4\n",
      "----------------------------------------------------------------------------------------------------\n",
      "parsing file >>>>>>>>>>>>>>>  5\n",
      "----------------------------------------------------------------------------------------------------\n",
      "parsing file >>>>>>>>>>>>>>>  6\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "Size of training data: 90000\n",
      "Size of testing data: 18000\n",
      "====================================================================================================\n",
      "top 10 frequent words:\n",
      "[('系列', 88969), ('月', 77879), ('中', 70570), ('年', 64838), ('产品', 61786), ('日', 58050), ('英寸', 57891), ('华硕', 56191), ('屏幕尺寸', 53018), ('主频', 52638)]\n",
      "Total vocabulary size:300000\n",
      "====================================================================================================\n",
      "dynamic_rnn inputs type: <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "dynamic_rnn inputs shape: (64, ?, 128)\n",
      "====================================================================================================\n",
      "dynamic_rnn output type: <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "dynamic_rnn output shape: (64, ?, 128)\n",
      "dynamic_rnn last_state type: <class 'tuple'>\n",
      "last_state: (LSTMStateTuple(c=<tf.Tensor 'RNNs_model/rnn/while/Exit_2:0' shape=(64, 128) dtype=float32>, h=<tf.Tensor 'RNNs_model/rnn/while/Exit_3:0' shape=(64, 128) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'RNNs_model/rnn/while/Exit_4:0' shape=(64, 128) dtype=float32>, h=<tf.Tensor 'RNNs_model/rnn/while/Exit_5:0' shape=(64, 128) dtype=float32>), LSTMStateTuple(c=<tf.Tensor 'RNNs_model/rnn/while/Exit_6:0' shape=(64, 128) dtype=float32>, h=<tf.Tensor 'RNNs_model/rnn/while/Exit_7:0' shape=(64, 128) dtype=float32>))\n",
      "====================================================================================================\n",
      "tf.shape(outputs): Tensor(\"RNNs_model/Shape_2:0\", shape=(3,), dtype=int32)\n",
      "batch_range: Tensor(\"RNNs_model/range_2:0\", shape=(?,), dtype=int32)\n",
      "indices Tensor(\"RNNs_model/stack:0\", shape=(64, 2), dtype=int32)\n",
      "====================================================================================================\n",
      "dynamic_rnn last_output type: <class 'tensorflow.python.framework.ops.Tensor'>\n",
      "dynamic_rnn last_output shape: (64, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\RECHAOS-012\\Anaconda_2.7\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1c08a1e48af8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# for training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextClassificationModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-c99efa15417c>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data, model, args)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_output\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m                 \u001b[0maaa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp_accuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp_cost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmp_summary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_euqal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m                 \u001b[0mtrain_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtmp_summary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda_2.7\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda_2.7\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda_2.7\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m~\\Anaconda_2.7\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda_2.7\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = Arguments()\n",
    "    data = DataGenerator(args)\n",
    "    \n",
    "    # for training\n",
    "    model = TextClassificationModel(args)\n",
    "    train(data, model, args)\n",
    "    \n",
    "    \n",
    "    # after training model, testing it using whole testing data\n",
    "    # for testing\n",
    "    # model = TextClassificationModel(args, is_training=False)\n",
    "    # test(data, model, args) # predict_accuracy:0.769333\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
