{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code version of [sequence labeling - NER](https://github.com/gaoisbest/NLP-Projects/tree/master/Sequence%20labeling%20-%20NER), please click the link for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load conlleval.py\n",
    "# Python version of the evaluation script from CoNLL'00-\n",
    "# Originates from: https://github.com/spyysalo/conlleval.py\n",
    "\n",
    "\n",
    "# Intentional differences:\n",
    "# - accept any space as delimiter by default\n",
    "# - optional file argument (default STDIN)\n",
    "# - option to set boundary (-b argument)\n",
    "# - LaTeX output (-l argument) not supported\n",
    "# - raw tags (-r argument) not supported\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import codecs\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "ANY_SPACE = '<SPACE>'\n",
    "\n",
    "\n",
    "class FormatError(Exception):\n",
    "    pass\n",
    "\n",
    "Metrics = namedtuple('Metrics', 'tp fp fn prec rec fscore')\n",
    "\n",
    "\n",
    "class EvalCounts(object):\n",
    "    def __init__(self):\n",
    "        self.correct_chunk = 0    # number of correctly identified chunks\n",
    "        self.correct_tags = 0     # number of correct chunk tags\n",
    "        self.found_correct = 0    # number of chunks in corpus\n",
    "        self.found_guessed = 0    # number of identified chunks\n",
    "        self.token_counter = 0    # token counter (ignores sentence breaks)\n",
    "\n",
    "        # counts by type\n",
    "        self.t_correct_chunk = defaultdict(int)\n",
    "        self.t_found_correct = defaultdict(int)\n",
    "        self.t_found_guessed = defaultdict(int)\n",
    "\n",
    "\n",
    "def parse_args(argv):\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='evaluate tagging results using CoNLL criteria',\n",
    "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
    "    )\n",
    "    arg = parser.add_argument\n",
    "    arg('-b', '--boundary', metavar='STR', default='-X-',\n",
    "        help='sentence boundary')\n",
    "    arg('-d', '--delimiter', metavar='CHAR', default=ANY_SPACE,\n",
    "        help='character delimiting items in input')\n",
    "    arg('-o', '--otag', metavar='CHAR', default='O',\n",
    "        help='alternative outside tag')\n",
    "    arg('file', nargs='?', default=None)\n",
    "    return parser.parse_args(argv)\n",
    "\n",
    "\n",
    "def parse_tag(t):\n",
    "    m = re.match(r'^([^-]*)-(.*)$', t)\n",
    "    return m.groups() if m else (t, '')\n",
    "\n",
    "\n",
    "def evaluate(iterable_sss, options=None):\n",
    "    if options is None:\n",
    "        options = parse_args([])    # use defaults\n",
    "\n",
    "    counts = EvalCounts()\n",
    "    num_features = None       # number of features per line\n",
    "    in_correct = False        # currently processed chunks is correct until now\n",
    "    last_correct = 'O'        # previous chunk tag in corpus\n",
    "    last_correct_type = ''    # type of previously identified chunk tag\n",
    "    last_guessed = 'O'        # previously identified chunk tag\n",
    "    last_guessed_type = ''    # type of previous chunk tag in corpus\n",
    "\n",
    "    #print('iterable_sss type:')\n",
    "    #print(type(iterable_sss))\n",
    "\n",
    "    for line in iterable_sss:\n",
    "        line = line.rstrip('\\r\\n')\n",
    "\n",
    "        if options.delimiter == ANY_SPACE:\n",
    "            features = line.split()\n",
    "        else:\n",
    "            features = line.split(options.delimiter)\n",
    "\n",
    "        if num_features is None:\n",
    "            num_features = len(features)\n",
    "        elif num_features != len(features) and len(features) != 0:\n",
    "            raise FormatError('unexpected number of features: %d (%d)' %\n",
    "                              (len(features), num_features))\n",
    "\n",
    "        if len(features) == 0 or features[0] == options.boundary:\n",
    "            features = [options.boundary, 'O', 'O']\n",
    "        if len(features) < 3:\n",
    "            raise FormatError('unexpected number of features in line %s' % line)\n",
    "\n",
    "        guessed, guessed_type = parse_tag(features.pop())\n",
    "        correct, correct_type = parse_tag(features.pop())\n",
    "        first_item = features.pop(0)\n",
    "\n",
    "        if first_item == options.boundary:\n",
    "            guessed = 'O'\n",
    "\n",
    "        end_correct = end_of_chunk(last_correct, correct,\n",
    "                                   last_correct_type, correct_type)\n",
    "        end_guessed = end_of_chunk(last_guessed, guessed,\n",
    "                                   last_guessed_type, guessed_type)\n",
    "        start_correct = start_of_chunk(last_correct, correct,\n",
    "                                       last_correct_type, correct_type)\n",
    "        start_guessed = start_of_chunk(last_guessed, guessed,\n",
    "                                       last_guessed_type, guessed_type)\n",
    "\n",
    "        if in_correct:\n",
    "            if (end_correct and end_guessed and\n",
    "                last_guessed_type == last_correct_type):\n",
    "                in_correct = False\n",
    "                counts.correct_chunk += 1\n",
    "                counts.t_correct_chunk[last_correct_type] += 1\n",
    "            elif (end_correct != end_guessed or guessed_type != correct_type):\n",
    "                in_correct = False\n",
    "\n",
    "        if start_correct and start_guessed and guessed_type == correct_type:\n",
    "            in_correct = True\n",
    "\n",
    "        if start_correct:\n",
    "            counts.found_correct += 1\n",
    "            counts.t_found_correct[correct_type] += 1\n",
    "        if start_guessed:\n",
    "            counts.found_guessed += 1\n",
    "            counts.t_found_guessed[guessed_type] += 1\n",
    "        if first_item != options.boundary:\n",
    "            if correct == guessed and guessed_type == correct_type:\n",
    "                counts.correct_tags += 1\n",
    "            counts.token_counter += 1\n",
    "\n",
    "        last_guessed = guessed\n",
    "        last_correct = correct\n",
    "        last_guessed_type = guessed_type\n",
    "        last_correct_type = correct_type\n",
    "\n",
    "    if in_correct:\n",
    "        counts.correct_chunk += 1\n",
    "        counts.t_correct_chunk[last_correct_type] += 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "\n",
    "def uniq(iterable):\n",
    "  seen = set()\n",
    "  return [i for i in iterable if not (i in seen or seen.add(i))]\n",
    "\n",
    "\n",
    "def calculate_metrics(correct, guessed, total):\n",
    "    tp, fp, fn = correct, guessed-correct, total-correct\n",
    "    p = 0 if tp + fp == 0 else 1.*tp / (tp + fp)\n",
    "    r = 0 if tp + fn == 0 else 1.*tp / (tp + fn)\n",
    "    f = 0 if p + r == 0 else 2 * p * r / (p + r)\n",
    "    return Metrics(tp, fp, fn, p, r, f)\n",
    "\n",
    "\n",
    "def metrics(counts):\n",
    "    c = counts\n",
    "    overall = calculate_metrics(\n",
    "        c.correct_chunk, c.found_guessed, c.found_correct\n",
    "    )\n",
    "    by_type = {}\n",
    "    for t in uniq(list(c.t_found_correct) + list(c.t_found_guessed)):\n",
    "        by_type[t] = calculate_metrics(\n",
    "            c.t_correct_chunk[t], c.t_found_guessed[t], c.t_found_correct[t]\n",
    "        )\n",
    "    return overall, by_type\n",
    "\n",
    "\n",
    "def report(counts, out=None):\n",
    "    if out is None:\n",
    "        out = sys.stdout\n",
    "\n",
    "    overall, by_type = metrics(counts)\n",
    "\n",
    "    c = counts\n",
    "    out.write('processed %d tokens with %d phrases; ' %\n",
    "              (c.token_counter, c.found_correct))\n",
    "    out.write('found: %d phrases; correct: %d.\\n' %\n",
    "              (c.found_guessed, c.correct_chunk))\n",
    "\n",
    "    if c.token_counter > 0:\n",
    "        out.write('accuracy: %6.2f%%; ' %\n",
    "                  (100.*c.correct_tags/c.token_counter))\n",
    "        out.write('precision: %6.2f%%; ' % (100.*overall.prec))\n",
    "        out.write('recall: %6.2f%%; ' % (100.*overall.rec))\n",
    "        out.write('FB1: %6.2f\\n' % (100.*overall.fscore))\n",
    "\n",
    "    for i, m in sorted(by_type.items()):\n",
    "        out.write('%17s: ' % i)\n",
    "        out.write('precision: %6.2f%%; ' % (100.*m.prec))\n",
    "        out.write('recall: %6.2f%%; ' % (100.*m.rec))\n",
    "        out.write('FB1: %6.2f  %d\\n' % (100.*m.fscore, c.t_found_guessed[i]))\n",
    "\n",
    "\n",
    "def report_notprint(counts, out=None):\n",
    "    if out is None:\n",
    "        out = sys.stdout\n",
    "\n",
    "    overall, by_type = metrics(counts)\n",
    "\n",
    "    c = counts\n",
    "    final_report = []\n",
    "    line = []\n",
    "    line.append('processed %d tokens with %d phrases; ' %\n",
    "              (c.token_counter, c.found_correct))\n",
    "    line.append('found: %d phrases; correct: %d.\\n' %\n",
    "              (c.found_guessed, c.correct_chunk))\n",
    "    final_report.append(\"\".join(line))\n",
    "\n",
    "    if c.token_counter > 0:\n",
    "        line = []\n",
    "        line.append('accuracy: %6.2f%%; ' %\n",
    "                  (100.*c.correct_tags/c.token_counter))\n",
    "        line.append('precision: %6.2f%%; ' % (100.*overall.prec))\n",
    "        line.append('recall: %6.2f%%; ' % (100.*overall.rec))\n",
    "        line.append('FB1: %6.2f\\n' % (100.*overall.fscore))\n",
    "        final_report.append(\"\".join(line))\n",
    "\n",
    "    for i, m in sorted(by_type.items()):\n",
    "        line = []\n",
    "        line.append('%17s: ' % i)\n",
    "        line.append('precision: %6.2f%%; ' % (100.*m.prec))\n",
    "        line.append('recall: %6.2f%%; ' % (100.*m.rec))\n",
    "        line.append('FB1: %6.2f  %d\\n' % (100.*m.fscore, c.t_found_guessed[i]))\n",
    "        final_report.append(\"\".join(line))\n",
    "    return final_report\n",
    "\n",
    "\n",
    "def end_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    # check if a chunk ended between the previous and current word\n",
    "    # arguments: previous and current chunk tags, previous and current types\n",
    "    chunk_end = False\n",
    "\n",
    "    if prev_tag == 'E': chunk_end = True\n",
    "    if prev_tag == 'S': chunk_end = True\n",
    "\n",
    "    if prev_tag == 'B' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'B' and tag == 'O': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'B': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'S': chunk_end = True\n",
    "    if prev_tag == 'I' and tag == 'O': chunk_end = True\n",
    "\n",
    "    if prev_tag != 'O' and prev_tag != '.' and prev_type != type_:\n",
    "        chunk_end = True\n",
    "\n",
    "    # these chunks are assumed to have length 1\n",
    "    if prev_tag == ']': chunk_end = True\n",
    "    if prev_tag == '[': chunk_end = True\n",
    "\n",
    "    return chunk_end\n",
    "\n",
    "\n",
    "def start_of_chunk(prev_tag, tag, prev_type, type_):\n",
    "    # check if a chunk started between the previous and current word\n",
    "    # arguments: previous and current chunk tags, previous and current types\n",
    "    chunk_start = False\n",
    "\n",
    "    if tag == 'B': chunk_start = True\n",
    "    if tag == 'S': chunk_start = True\n",
    "\n",
    "    if prev_tag == 'E' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'E' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'S' and tag == 'I': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'E': chunk_start = True\n",
    "    if prev_tag == 'O' and tag == 'I': chunk_start = True\n",
    "\n",
    "    if tag != 'O' and tag != '.' and prev_type != type_:\n",
    "        chunk_start = True\n",
    "\n",
    "    # these chunks are assumed to have length 1\n",
    "    if tag == '[': chunk_start = True\n",
    "    if tag == ']': chunk_start = True\n",
    "\n",
    "    return chunk_start\n",
    "\n",
    "\n",
    "def return_report(input_file):\n",
    "    with codecs.open(input_file, \"r\", \"utf8\") as f:\n",
    "        counts = evaluate(f)\n",
    "    return report_notprint(counts)\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    args = parse_args(argv[1:])\n",
    "\n",
    "    if args.file is None:\n",
    "        counts = evaluate(sys.stdin, args)\n",
    "    else:\n",
    "        with open(args.file) as f:\n",
    "            counts = evaluate(f, args)\n",
    "    report(counts)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    sys.exit(main(sys.argv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load utils_io.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import shutil\n",
    "import jieba as jie\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "\n",
    "def iob2(tags):\n",
    "    \"\"\"\n",
    "    Check that tags have a valid IOB format.\n",
    "    Tags in IOB1 format are converted to IOB2.\n",
    "    \"\"\"\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            continue\n",
    "        split = tag.split('-')\n",
    "        if len(split) != 2 or split[0] not in ['I', 'B']:\n",
    "            return False\n",
    "        if split[0] == 'B':\n",
    "            continue\n",
    "        elif i == 0 or tags[i - 1] == 'O':  # conversion IOB1 to IOB2\n",
    "            tags[i] = 'B' + tag[1:]\n",
    "        elif tags[i - 1][1:] == tag[1:]:\n",
    "            continue\n",
    "        else:  # conversion IOB1 to IOB2\n",
    "            tags[i] = 'B' + tag[1:]\n",
    "    return True\n",
    "\n",
    "def iob_iobes(tags):\n",
    "    \"\"\"\n",
    "    IOB -> IOBES\n",
    "    \"\"\"\n",
    "    new_tags = []\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            new_tags.append(tag)\n",
    "        elif tag.split('-')[0] == 'B':\n",
    "            if i + 1 != len(tags) and \\\n",
    "               tags[i + 1].split('-')[0] == 'I':\n",
    "                new_tags.append(tag)\n",
    "            else:\n",
    "                new_tags.append(tag.replace('B-', 'S-'))\n",
    "        elif tag.split('-')[0] == 'I':\n",
    "            if i + 1 < len(tags) and \\\n",
    "                    tags[i + 1].split('-')[0] == 'I':\n",
    "                new_tags.append(tag)\n",
    "            else:\n",
    "                new_tags.append(tag.replace('I-', 'E-'))\n",
    "        else:\n",
    "            raise Exception('Invalid IOB format!')\n",
    "    return new_tags\n",
    "\n",
    "def full_to_half(s):\n",
    "    \"\"\"\n",
    "    Convert full-width character to half-width one \n",
    "    \"\"\"\n",
    "    n = []\n",
    "    for char in s:\n",
    "        num = ord(char)\n",
    "        if num == 0x3000:\n",
    "            num = 32\n",
    "        elif 0xFF01 <= num <= 0xFF5E:\n",
    "            num -= 0xfee0\n",
    "        char = chr(num)\n",
    "        n.append(char)\n",
    "    return ''.join(n)\n",
    "\n",
    "def replace_html(s):\n",
    "    s = s.replace('&quot;','\"')\n",
    "    s = s.replace('&amp;','&')\n",
    "    s = s.replace('&lt;','<')\n",
    "    s = s.replace('&gt;','>')\n",
    "    s = s.replace('&nbsp;',' ')\n",
    "    s = s.replace(\"&ldquo;\", \"“\")\n",
    "    s = s.replace(\"&rdquo;\", \"”\")\n",
    "    s = s.replace(\"&mdash;\",\"\")\n",
    "    s = s.replace(\"\\xa0\", \" \")\n",
    "    return(s)\n",
    "\n",
    "def input_from_line(line, char_to_id):\n",
    "    \"\"\"\n",
    "    Take sentence data and return an input for\n",
    "    the training or the evaluation function.\n",
    "    \"\"\"\n",
    "    line = full_to_half(line)\n",
    "    line = replace_html(line)\n",
    "    inputs = list()\n",
    "    inputs.append([line])\n",
    "    line.replace(' ', '$')\n",
    "    inputs.append([[char_to_id[char] if char in char_to_id else char_to_id['<UNK>'] for char in line]])\n",
    "    inputs.append([get_seg_features(line)])\n",
    "    inputs.append([[]])\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def get_seg_features(string):\n",
    "    \"\"\"\n",
    "    Chinese word segmentation with jieba.\n",
    "    Features are represented in BIES format, i.e., B:1, I:2, E:3, S:0.\n",
    "\n",
    "    For example:\n",
    "    string: u'我买了富士康手机'\n",
    "    encoding: [0, 0, 0, 1, 2, 3, 1, 3]    \n",
    "    \"\"\"\n",
    "    seg_feature = []\n",
    "\n",
    "    for word in jie.cut(string):\n",
    "        if len(word) == 1:\n",
    "            seg_feature.append(0)\n",
    "        else:\n",
    "            tmp = [2] * len(word)\n",
    "            tmp[0] = 1\n",
    "            tmp[-1] = 3\n",
    "            seg_feature.extend(tmp)\n",
    "    return seg_feature\n",
    "\n",
    "\n",
    "def result_to_json(string, tags):\n",
    "    item = {'string': string, 'entities': []}\n",
    "    entity_name = \"\"\n",
    "    entity_start = 0\n",
    "    idx = 0\n",
    "    for char, tag in zip(string, tags):\n",
    "        if tag[0] == 'S':\n",
    "            item['entities'].append({'word': char, 'start': idx, 'end': idx+1, 'type':tag[2:]})\n",
    "        elif tag[0] == 'B':\n",
    "            entity_name += char\n",
    "            entity_start = idx\n",
    "        elif tag[0] == 'I':\n",
    "            entity_name += char\n",
    "        elif tag[0] == 'E':\n",
    "            entity_name += char\n",
    "            item['entities'].append({'word': entity_name, 'start': entity_start, 'end': idx + 1, 'type': tag[2:]})\n",
    "            entity_name = \"\"\n",
    "        else:\n",
    "            entity_name = \"\"\n",
    "            entity_start = idx\n",
    "        idx += 1\n",
    "    return item\n",
    "\n",
    "def save_config(config, config_file):\n",
    "    \"\"\"\n",
    "    Save configuration of the model parameters for model deploy.\n",
    "    Parameters are stored in json format.\n",
    "    \"\"\"\n",
    "    with open(config_file, mode='w', encoding='utf8') as f:\n",
    "        json.dump(config, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "def load_config(config_file):\n",
    "    \"\"\"\n",
    "    Load configuration of the model for model deploy.    \n",
    "    Parameters are stored in json format.\n",
    "    \"\"\"\n",
    "    with open(config_file, encoding='utf8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def load_word2vec(emb_path, id_to_word, word_dim, old_weights):\n",
    "    \"\"\"\n",
    "    Load word embedding from pre-trained file.\n",
    "    \"\"\"\n",
    "    new_weights = old_weights\n",
    "    print('Loading pretrained embeddings from {}...'.format(emb_path))\n",
    "    pre_trained = {}\n",
    "    emb_invalid = 0\n",
    "    for i, line in enumerate(codecs.open(emb_path, 'r', 'utf-8')):\n",
    "        # line format: [character_a, 0.0824, -0.335, ..., 'word_dim' embeddings]\n",
    "        line = line.rstrip().split()\n",
    "        if len(line) == word_dim + 1: # valid embedding\n",
    "            # line[0] is a character\n",
    "            pre_trained[line[0]] = np.array([float(x) for x in line[1:]]).astype(np.float32)\n",
    "        else:\n",
    "            emb_invalid += 1\n",
    "    if emb_invalid > 0:\n",
    "        print('WARNING: %i invalid lines' % emb_invalid)\n",
    "\n",
    "    c_found = 0\n",
    "    c_lower = 0\n",
    "    c_zeros = 0\n",
    "    n_words = len(id_to_word)\n",
    "    # Lookup table initialization\n",
    "    for i in range(n_words):\n",
    "        word = id_to_word[i]\n",
    "        if word in pre_trained:\n",
    "            new_weights[i] = pre_trained[word]\n",
    "            c_found += 1\n",
    "        elif word.lower() in pre_trained:\n",
    "            new_weights[i] = pre_trained[word.lower()]\n",
    "            c_lower += 1\n",
    "        elif re.sub('\\d', '0', word.lower()) in pre_trained:\n",
    "            new_weights[i] = pre_trained[re.sub('\\d', '0', word.lower())]\n",
    "            c_zeros += 1\n",
    "    print('Loaded %i pre-trained embeddings.' % len(pre_trained))\n",
    "    print('%i / %i (%.4f%%) words have been initialized with pretrained embeddings.' % (c_found + c_lower + c_zeros, n_words,\n",
    "        100. * (c_found + c_lower + c_zeros) / n_words))\n",
    "    print('%i found directly, %i after lowercasing, %i after lowercasing + zero.' % (c_found, c_lower, c_zeros))\n",
    "    return new_weights\n",
    "\n",
    "\n",
    "def make_path(params):\n",
    "    \"\"\"\n",
    "    Make folders for training and evaluation\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(params.summary_dir):\n",
    "        os.makedirs(params.summary_dir)\n",
    "    if not os.path.isdir(params.ckpt_dir):\n",
    "        os.makedirs(params.ckpt_dir)\n",
    "\n",
    "\n",
    "def tag_mapping(sentences):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of tags, sorted by counts.\n",
    "    \"\"\"\n",
    "    # x[0]: character\n",
    "    # x[-1]: tag\n",
    "    # tags is a list of list which only contain x[-1] (i.e., tag)\n",
    "    tags = [[x[-1] for x in s] for s in sentences]\n",
    "    dico = create_dictionary(tags)\n",
    "    tag_to_id, id_to_tag = create_mapping(dico)\n",
    "    print('Found %i unique named entity tags.' % len(dico))\n",
    "    return dico, tag_to_id, id_to_tag\n",
    "\n",
    "def char_mapping(sentences, lower):\n",
    "    \"\"\"\n",
    "    Create a dictionary and a mapping of characters, sorted by counts.\n",
    "    \"\"\"\n",
    "    # x[0]: character\n",
    "    # x[1]: tag\n",
    "    # chars is a list of list which only contain x[0] (i.e., character)\n",
    "    chars = [[x[0].lower() if lower else x[0] for x in s] for s in sentences]\n",
    "    dico = create_dictionary(chars)\n",
    "    # add two extra characters\n",
    "    dico['<PAD>'] = 10000001 # padding symbol, 10000001 is counts\n",
    "    dico['<UNK>'] = 10000000 # unknown symbol\n",
    "    char_to_id, id_to_char = create_mapping(dico)\n",
    "    print('Found %i unique words (%i in total).' % (len(dico), sum(len(x) for x in chars)))\n",
    "    return dico, char_to_id, id_to_char\n",
    "\n",
    "def augment_with_pretrained(dictionary, ext_emb_path, chars):\n",
    "    \"\"\"\n",
    "    Augment the dictionary with chars that have a pre-trained embedding.  \n",
    "    \"\"\"\n",
    "    print('Loading pretrained embeddings from %s...' % ext_emb_path)\n",
    "    assert os.path.isfile(ext_emb_path)\n",
    "\n",
    "    # 'pretrained' contains characters that have pre-trained embedding vector\n",
    "    # line format: character_a 0.0824 -0.335 ... 100 embeddings\n",
    "    pretrained = set([line.rstrip().split()[0].strip() for line in codecs.open(ext_emb_path, 'r', 'utf-8') if len(ext_emb_path) > 0])\n",
    "\n",
    "    if chars is None: # add every character that has a pre-trained embedding to the dictionary\n",
    "        for char in pretrained:\n",
    "            if char not in dictionary:\n",
    "                dictionary[char] = 0\n",
    "    else: # add the words that are given by `chars` (typically the words in the development and test sets.)\n",
    "        for char in chars:\n",
    "            if any(x in pretrained for x in [char, char.lower(), re.sub('\\d', '0', char.lower())]) and char not in dictionary:\n",
    "                dictionary[char] = 0\n",
    "\n",
    "    char_to_id, id_to_char = create_mapping(dictionary)\n",
    "    return dictionary, char_to_id, id_to_char\n",
    "\n",
    "def create_dictionary(item_list):\n",
    "    \"\"\"\n",
    "    Create the dictionary[character] = character counts.\n",
    "    \"\"\"\n",
    "    assert type(item_list) is list\n",
    "    dico = {}\n",
    "    # items is a list, which represents the sentence, that contains each character, i.e., ['a', 'b', ...]\n",
    "    for items in item_list:\n",
    "        for item in items: # item is each character\n",
    "            if item not in dico:\n",
    "                dico[item] = 1\n",
    "            else:\n",
    "                dico[item] += 1\n",
    "    return dico\n",
    "\n",
    "def create_mapping(dico):\n",
    "    \"\"\"\n",
    "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
    "    Items are ordered by decreasing frequency.\n",
    "    \"\"\"\n",
    "    # sort the dictionary in descending order\n",
    "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
    "    # assign id to each character, i.e., id:character\n",
    "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
    "    # character:id\n",
    "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
    "    return item_to_id, id_to_item\n",
    "\n",
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    Replace every digit in a string by a zero.\n",
    "    \"\"\"\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n",
    "def load_sentences(path, zeros):\n",
    "    \"\"\"\n",
    "    Read character-based input data. \n",
    "    [1] A line must contain at least a character and its tag.\n",
    "    [2] Sentences are separated by empty line.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    num = 0\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        num += 1\n",
    "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            if line[0] == \" \":\n",
    "                line = \"$\" + line[1:]\n",
    "                word = line.split()\n",
    "            else:\n",
    "                word= line.split()\n",
    "            if len(word) >= 2:\n",
    "                sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def clean(params, maps_file_name, config_file_name, results_file_path):\n",
    "    \"\"\"\n",
    "    Clean current folder\n",
    "    remove saved model and training log\n",
    "    \"\"\"\n",
    "    if os.path.isfile(os.path.join(params.data_dir, maps_file_name)):\n",
    "        os.remove(os.path.join(params.data_dir, maps_file_name))\n",
    "\n",
    "    if os.path.isdir(params.ckpt_dir):\n",
    "        shutil.rmtree(params.ckpt_dir)\n",
    "\n",
    "    if os.path.isdir(params.summary_dir):\n",
    "        shutil.rmtree(params.summary_dir)\n",
    "\n",
    "    if os.path.isdir(os.path.join(params.data_dir, results_file_path)):\n",
    "        shutil.rmtree(os.path.join(params.data_dir, results_file_path))\n",
    "\n",
    "    if os.path.isdir('__pycache__'):\n",
    "        shutil.rmtree('__pycache__')\n",
    "\n",
    "    if os.path.isfile(os.path.join(params.data_dir, config_file_name)):\n",
    "        os.remove(os.path.join(params.data_dir, config_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load utils_train.py\n",
    "\n",
    "import os\n",
    "import re\n",
    "import codecs\n",
    "import shutil\n",
    "import jieba as jie\n",
    "import random\n",
    "import math\n",
    "import logging\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def update_tag_scheme(sentences, tag_scheme):\n",
    "    \"\"\"\n",
    "    Check and update sentences tagging scheme to tag_scheme.\n",
    "    Only IOB1 and IOB2 schemes are accepted.\n",
    "    \"\"\"\n",
    "    for i, s in enumerate(sentences):\n",
    "        tags = [w[-1] for w in s]\n",
    "        # Check that tags are given in the IOB format\n",
    "        if not iob2(tags):\n",
    "            s_str = '\\n'.join(' '.join(w) for w in s)\n",
    "            raise Exception('Sentences should be given in IOB format! ' +\n",
    "                            'Please check sentence %i:\\n%s' % (i, s_str))\n",
    "        if tag_scheme == 'iob':\n",
    "            # If format was IOB1, we convert to IOB2\n",
    "            for word, new_tag in zip(s, tags):\n",
    "                word[-1] = new_tag\n",
    "        elif tag_scheme == 'iobes':\n",
    "            new_tags = iob_iobes(tags)\n",
    "            for word, new_tag in zip(s, new_tags):\n",
    "                word[-1] = new_tag\n",
    "        else:\n",
    "            raise Exception('Unknown tagging scheme!')\n",
    "\n",
    "def iob2(tags):\n",
    "    \"\"\"\n",
    "    Check that tags have a valid IOB format.\n",
    "    Tags in IOB1 format are converted to IOB2.\n",
    "    \"\"\"\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            continue\n",
    "        split = tag.split('-')\n",
    "        if len(split) != 2 or split[0] not in ['I', 'B']:\n",
    "            return False\n",
    "        if split[0] == 'B':\n",
    "            continue\n",
    "        elif i == 0 or tags[i - 1] == 'O':  # conversion IOB1 to IOB2\n",
    "            tags[i] = 'B' + tag[1:]\n",
    "        elif tags[i - 1][1:] == tag[1:]:\n",
    "            continue\n",
    "        else:  # conversion IOB1 to IOB2\n",
    "            tags[i] = 'B' + tag[1:]\n",
    "    return True\n",
    "\n",
    "def iobes_iob(tags):\n",
    "    \"\"\"\n",
    "    IOBES -> IOB\n",
    "    \"\"\"\n",
    "    new_tags = []\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag.split('-')[0] == 'B':\n",
    "            new_tags.append(tag)\n",
    "        elif tag.split('-')[0] == 'I':\n",
    "            new_tags.append(tag)\n",
    "        elif tag.split('-')[0] == 'S':\n",
    "            new_tags.append(tag.replace('S-', 'B-'))\n",
    "        elif tag.split('-')[0] == 'E':\n",
    "            new_tags.append(tag.replace('E-', 'I-'))\n",
    "        elif tag.split('-')[0] == 'O':\n",
    "            new_tags.append(tag)\n",
    "        else:\n",
    "            raise Exception('Invalid format!')\n",
    "    return new_tags\n",
    "\n",
    "def iob_iobes(tags):\n",
    "    \"\"\"\n",
    "    IOB -> IOBES\n",
    "    \"\"\"\n",
    "    new_tags = []\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag == 'O':\n",
    "            new_tags.append(tag)\n",
    "        elif tag.split('-')[0] == 'B':\n",
    "            if i + 1 != len(tags) and \\\n",
    "               tags[i + 1].split('-')[0] == 'I':\n",
    "                new_tags.append(tag)\n",
    "            else:\n",
    "                new_tags.append(tag.replace('B-', 'S-'))\n",
    "        elif tag.split('-')[0] == 'I':\n",
    "            if i + 1 < len(tags) and \\\n",
    "                    tags[i + 1].split('-')[0] == 'I':\n",
    "                new_tags.append(tag)\n",
    "            else:\n",
    "                new_tags.append(tag.replace('I-', 'E-'))\n",
    "        else:\n",
    "            raise Exception('Invalid IOB format!')\n",
    "    return new_tags\n",
    "\n",
    "class BatchManager(object):\n",
    "    def __init__(self, data,  batch_size):\n",
    "        # data format: [[string, chars, segs, tags], [next sentence information]]\n",
    "        self.batch_data = self.sort_and_pad(data, batch_size)\n",
    "        self.len_data = len(self.batch_data)\n",
    "\n",
    "    def sort_and_pad(self, data, batch_size):\n",
    "        num_batch = int(math.ceil(len(data) / batch_size))\n",
    "        #print('len(data):{}, batch_size:{}, num_batch:{}'.format(len(data), batch_size, num_batch))\n",
    "        # for minimal padding, sort them according to sequence length\n",
    "        sorted_data = sorted(data, key=lambda x: len(x[0]))\n",
    "        batch_data = list()\n",
    "        for i in range(num_batch): # iterate the data\n",
    "            batch_data.append(self.pad_data(sorted_data[i*batch_size : (i+1)*batch_size]))\n",
    "        return batch_data\n",
    "\n",
    "    @staticmethod\n",
    "    def pad_data(data):\n",
    "        strings = []\n",
    "        chars = []\n",
    "        segs = []\n",
    "        targets = []\n",
    "        max_length = max([len(sentence[0]) for sentence in data])\n",
    "        for line in data:\n",
    "            string, char, seg, target = line\n",
    "            padding = [0] * (max_length - len(string))\n",
    "            strings.append(string + padding)\n",
    "            chars.append(char + padding)\n",
    "            segs.append(seg + padding)\n",
    "            targets.append(target + padding)\n",
    "        '''\n",
    "        take a look at first two training examples.\n",
    "        \n",
    "        print(strings[0:2])\n",
    "        [['一', '亿', '二', '的', '代', '价', '（', '中', '华', '环', '保', '世', '纪', '行', '）'], ['钱', '其', '琛', '会', '见', '香', '港', '泉', '州', '同', '乡', '会', '访', '问', '团']]\n",
    "        \n",
    "        print(chars[0:2])\n",
    "        [[7, 523, 283, 3, 122, 417, 306, 10, 244, 393, 138, 142, 374, 32, 307], [694, 147, 1966, 20, 319, 555, 514, 1283, 461, 66, 406, 20, 405, 123, 253]]\n",
    "        \n",
    "        print(segs[0:2])\n",
    "        [[1, 3, 0, 0, 1, 3, 0, 1, 3, 1, 3, 1, 3, 0, 0], [1, 2, 3, 1, 3, 1, 3, 1, 3, 1, 2, 3, 1, 2, 3]]\n",
    "        \n",
    "        print(targets[0:2])\n",
    "        [[0, 0, 0, 0, 0, 0, 0, 2, 3, 0, 0, 0, 0, 0, 0], [8, 7, 9, 0, 0, 4, 1, 1, 1, 1, 1, 1, 1, 1, 5]]\n",
    "        '''\n",
    "        return [strings, chars, segs, targets]\n",
    "\n",
    "    def iter_batch(self, shuffle=False):\n",
    "        if shuffle:\n",
    "            random.shuffle(self.batch_data)\n",
    "        for idx in range(self.len_data):\n",
    "            yield self.batch_data[idx]\n",
    "\n",
    "\n",
    "def prepare_dataset(sentences, char_to_id, tag_to_id, lower=False, train=True):\n",
    "    \"\"\"\n",
    "    Prepare the dataset. Return a list of lists and each inner list contains:\n",
    "    - string: character list\n",
    "    - chars: character id list\n",
    "    - segs: word segmentation encoding, see get_seg_features for more detail\n",
    "    - tags: tag id list\n",
    "    \"\"\"\n",
    "\n",
    "    none_index = tag_to_id['O']\n",
    "\n",
    "    def f(x):\n",
    "        return x.lower() if lower else x\n",
    "\n",
    "    data = []\n",
    "    # sentences is a list of list\n",
    "    for s in sentences: # s is a list\n",
    "        string = [w[0] for w in s]\n",
    "        # convert each char to its id\n",
    "        chars = [char_to_id[f(w) if f(w) in char_to_id else '<UNK>'] for w in string]\n",
    "        segs = get_seg_features(\"\".join(string))\n",
    "        if train:\n",
    "            # convert each tag to its id\n",
    "            tags = [tag_to_id[w[-1]] for w in s]\n",
    "        else:\n",
    "            tags = [none_index for _ in chars]\n",
    "        data.append([string, chars, segs, tags])\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_seg_features(string):\n",
    "    \"\"\"\n",
    "    Chinese word segmentation with jieba.\n",
    "    Features are represented in BIES format, i.e., B:1, I:2, E:3, S:0.\n",
    "    \n",
    "    For example:\n",
    "    string: u'我买了富士康手机'\n",
    "    encoding: [0, 0, 0, 1, 2, 3, 1, 3]    \n",
    "    \"\"\"\n",
    "    seg_feature = []\n",
    "\n",
    "    for word in jie.cut(string):\n",
    "        if len(word) == 1:\n",
    "            seg_feature.append(0)\n",
    "        else:\n",
    "            tmp = [2] * len(word)\n",
    "            tmp[0] = 1\n",
    "            tmp[-1] = 3\n",
    "            seg_feature.extend(tmp)\n",
    "    return seg_feature\n",
    "\n",
    "\n",
    "def zero_digits(s):\n",
    "    \"\"\"\n",
    "    Replace every digit in a string by a zero.\n",
    "    \"\"\"\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load model.py\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils_io import result_to_json\n",
    "from utils_train import iobes_iob\n",
    "\n",
    "\n",
    "class BrandsNERModel:\n",
    "    \"\"\"\n",
    "    Model for recognizing brands.    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.learning_rate = config['learning_rate']\n",
    "        self.char_dims = config['char_dim']\n",
    "        self.num_chars = config['num_chars']\n",
    "        self.word_dims = config['word_dim']\n",
    "        self.num_words_types = 4 # BIES\n",
    "        self.num_tags = config['num_tags']\n",
    "        self.rnn_units = config['num_units']\n",
    "        self.max_gradient_norm = config['max_gradient_norm']\n",
    "\n",
    "        # input placeholders\n",
    "        self.char_inputs = tf.placeholder(dtype=tf.int32, shape=[None, None], name='char_inputs')\n",
    "        self.word_inputs = tf.placeholder(dtype=tf.int32, shape=[None, None], name='word_inputs')\n",
    "        self.tags = tf.placeholder(dtype=tf.int32, shape=[None, None], name='brand_tags')\n",
    "        self.keep_prob = tf.placeholder(dtype=tf.float32, name='dropout_keep_prob')\n",
    "\n",
    "        self.batch_size = tf.shape(input=self.char_inputs)[0]\n",
    "        self.num_steps = tf.shape(input=self.char_inputs)[1]\n",
    "\n",
    "        self.global_step = tf.Variable(initial_value=0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "        # model's best development F1 score\n",
    "        self.best_dev_f1 = tf.Variable(initial_value=0.0, dtype=tf.float32, trainable=False, name='best_dev_f1')\n",
    "        # model's best test F1 score\n",
    "        self.best_test_f1 = tf.Variable(initial_value=0.0, dtype=tf.float32, trainable=False, name='best_test_f1')\n",
    "\n",
    "        # use crf or not\n",
    "        self.use_crf = config['use_crf']\n",
    "\n",
    "        # model architecture\n",
    "        # embedding layer\n",
    "        char_word_embeddings = self.embedding_layer(self.char_inputs, self.word_inputs)\n",
    "\n",
    "        # dropout\n",
    "        # according to 'Neural Architectures for Named Entity Recognition' Section 4.3\n",
    "        rnn_inputs = tf.nn.dropout(x=char_word_embeddings, keep_prob=self.keep_prob, name='lstm_inputs_dropout')\n",
    "\n",
    "        # get the actual sequence length of this batch\n",
    "        self.seq_lengths = tf.cast(tf.reduce_sum(input_tensor=tf.sign(tf.abs(self.char_inputs)), axis=1), tf.int32)\n",
    "\n",
    "        # bi-directional rnn layer\n",
    "        rnn_outputs = self.birnn_layer(rnn_inputs, self.rnn_units, self.seq_lengths)\n",
    "\n",
    "        self.logits = self.projection_layer(rnn_outputs)\n",
    "\n",
    "        self.cost = self.cost_layer(self.logits, self.seq_lengths, self.use_crf)\n",
    "\n",
    "        self.train_op = self.optimize(self.cost)\n",
    "\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    def embedding_layer(self, char_inputs, word_inputs):\n",
    "        \"\"\"\n",
    "        Character and word segmentation embedding.\n",
    "        :return: concatenated character and word segmentation embedding.\n",
    "        \"\"\"\n",
    "        char_and_word_embeddings = []\n",
    "        with tf.variable_scope('char_embedding_layer'), tf.device('/cpu:0'):\n",
    "            self.char_embeddings = tf.get_variable(name='char_embeddings', shape=[self.num_chars, self.char_dims], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            char_and_word_embeddings.append(tf.nn.embedding_lookup(params=self.char_embeddings, ids=char_inputs, name='char_embeddings_lookup'))\n",
    "        if self.word_dims > 0:\n",
    "            with tf.variable_scope('word_embedding_layer'), tf.device('/cpu:0'):\n",
    "                self.word_embeddings = tf.get_variable(name='word_embeddings', shape=[self.num_words_types, self.word_dims], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "                char_and_word_embeddings.append(tf.nn.embedding_lookup(params=self.word_embeddings, ids=word_inputs, name='word_embeddings_loopup'))\n",
    "        # shape of rtn_embeddings: [None, None, self.char_dims + self.word_dims]\n",
    "        # axis=2 is also ok\n",
    "        rtn_embeddings = tf.concat(values=char_and_word_embeddings, axis=-1)\n",
    "        return rtn_embeddings\n",
    "\n",
    "    def birnn_layer(self, rnn_inputs, rnn_num_units, seq_lengths):\n",
    "        \"\"\"\n",
    "        Bi-directional LSTM model.\n",
    "        :return: concatenated forward and backward outputs.\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('birnn_layer'):\n",
    "            rnn_cells = {}\n",
    "            for tmp in ['forward', 'backward']:\n",
    "                # according to 'Neural Architectures for Named Entity Recognition' Section 2.1\n",
    "                # CoupledInputForgetGateLSTMCell from paper 'LSTM: A Search Space Odyssey', where f = 1- i.\n",
    "                rnn_cells[tmp] = tf.contrib.rnn.CoupledInputForgetGateLSTMCell(num_units=rnn_num_units, use_peepholes=True, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            # obtain 'contextual word representation' through bi-rnn according to https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html\n",
    "            outputs, output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=rnn_cells['forward'], cell_bw=rnn_cells['backward'], inputs=rnn_inputs, sequence_length=seq_lengths, dtype=tf.float32)\n",
    "        # concatenate forward output and backward output: [None, None, self.rnn_units + self.rnn_units]\n",
    "        return tf.concat(values=outputs, axis=-1)\n",
    "\n",
    "    def projection_layer(self, rnn_outputs):\n",
    "        \"\"\"\n",
    "        Two hidden fully connected layers for computing tag scores for each character.\n",
    "        :param rnn_outputs: the outputs of birnn_layer \n",
    "        :return: tag scores matrix P \n",
    "        \"\"\"\n",
    "        with tf.variable_scope('projections'):\n",
    "            project_W = tf.get_variable(name='project_W', shape=[2 * self.rnn_units, self.rnn_units], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            # use tf.zeros_initializer()\n",
    "            project_b = tf.get_variable(name='project_b', shape=[self.rnn_units], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "            rnn_outputs_flat = tf.reshape(tensor=rnn_outputs, shape=[-1, 2 * self.rnn_units])\n",
    "            project_Z = tf.nn.xw_plus_b(x=rnn_outputs_flat, weights=project_W, biases=project_b, name='projection_Z')\n",
    "            # activation\n",
    "            project_A = tf.tanh(project_Z, name='projection_A')\n",
    "        with tf.variable_scope('logits'):\n",
    "            logits_W = tf.get_variable(name='logits_W', shape=[self.rnn_units, self.num_tags], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            logits_b = tf.get_variable(name='logits_b', shape=[self.num_tags], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "            logits = tf.nn.xw_plus_b(x=project_A, weights=logits_W, biases=logits_b, name='matrix_P')\n",
    "            logits_format = tf.reshape(tensor=logits, shape=[-1, self.num_steps, self.num_tags])\n",
    "\n",
    "        # I have tried only one single hidden layer, but the F1 score is lower about 0.3%, the codes likes:\n",
    "        '''\n",
    "        with tf.variable_scope('logits'):\n",
    "            logits_W = tf.get_variable(name='logits_W', shape=[2 * self.rnn_units, self.num_tags], dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            logits_b = tf.get_variable(name='logits_b', shape=[self.num_tags], dtype=tf.float32, initializer=tf.zeros_initializer())\n",
    "            rnn_outputs_flat = tf.reshape(tensor=rnn_outputs, shape=[-1, 2 * self.rnn_units])\n",
    "            logits = tf.nn.xw_plus_b(x=rnn_outputs_flat, weights=logits_W, biases=logits_b, name='matrix_P')\n",
    "            logits_format = tf.reshape(tensor=logits, shape=[-1, self.num_steps, self.num_tags])\n",
    "        '''\n",
    "        return logits_format\n",
    "\n",
    "    def cost_layer(self, logits, seq_lengths, use_crf):\n",
    "        \"\"\"\n",
    "        Model cost for crf layer and softmax layer.\n",
    "        \"\"\"\n",
    "        if use_crf:\n",
    "            with tf.variable_scope('crf_cost'):\n",
    "                # reference codes see https://github.com/glample/tagger/blob/master/model.py line 288\n",
    "                small = -1000.0\n",
    "                # let y_-1 (i.e., self.num_tags) and y_n(i.e., self.num_tag+1) be the start and end tags of the sequence, respectively.\n",
    "                # shape of seq_start_logits: [self.batch_size, 1, self.num_tags + 2], here 2 means the start and end tags\n",
    "                # let seq_0 be the added start character of the sequence, then the ner tag of seq_0 is y_-1, since the second last 0 (tf.zeros()) > -1000.0 (small * tf.ones())\n",
    "                seq_start_logits = tf.concat(values=[small * tf.ones(shape=[self.batch_size, 1, self.num_tags]), tf.zeros(shape=[self.batch_size, 1, 1]), small * tf.ones(shape=[self.batch_size, 1, 1])], axis=-1)\n",
    "\n",
    "                # shape of seq_end_logits: [self.batch_size, 1, self.num_tags+2]\n",
    "                # let seq_n be the added end character of the sequence, then the ner tag of seq_n is y_n, since the last 0 (tf.zeros()) > -1000.0 (small * tf.ones())\n",
    "                #seq_end_logits = tf.concat(values=[small * tf.ones(shape=[self.batch_size, 1, self.num_tags + 1]), tf.zeros(shape=[self.batch_size, 1, 1])], axis=-1)\n",
    "                seq_end_logits = tf.concat(values=[small * tf.ones(shape=[self.batch_size, 1, self.num_tags]), small * tf.ones(shape=[self.batch_size, 1, 1]), tf.zeros(shape=[self.batch_size, 1, 1])], axis=-1)\n",
    "\n",
    "                # padding the logits with small values, small values make sure that padding will not affect acutal predicted values.\n",
    "                # shape of padded_logits: [self.batch_size, self.num_steps, self.num_tags + 2]\n",
    "                padded_logits = tf.concat(values=[logits, tf.cast(small * tf.ones([self.batch_size, self.num_steps, 2]), tf.float32)], axis=-1)\n",
    "\n",
    "                # shape of final_logits: [self.batch_size, self.num_steps + 2, self.num_tags + 2]\n",
    "                final_logits = tf.concat(values=[seq_start_logits, padded_logits, seq_end_logits], axis=1)\n",
    "\n",
    "                # padding the actual tags\n",
    "                # the ner tag of padded seq_0 and seq_n is self.num_tags and self.num_tags+1, respectively\n",
    "                # shape of padded final_tags: [self.batch_size, self.num_steps + 2]\n",
    "                seq_start_tags = tf.cast(self.num_tags * tf.ones(shape=[self.batch_size, 1]), tf.int32)\n",
    "                seq_end_tags = tf.cast((self.num_tags+1) * tf.ones(shape=[self.batch_size, 1]), tf.int32)\n",
    "                final_tags = tf.concat(values=[seq_start_tags, self.tags, seq_end_tags], axis=-1)\n",
    "\n",
    "                log_likelihood, self.transition_matrix = tf.contrib.crf.crf_log_likelihood(inputs=final_logits, tag_indices=final_tags, sequence_lengths=seq_lengths+2)\n",
    "                crf_cost = tf.reduce_mean(-log_likelihood)\n",
    "\n",
    "                tf.summary.scalar(name='cost', tensor=crf_cost)\n",
    "                tf.summary.histogram(name='histogram_cost', values=crf_cost)\n",
    "                self.summary_op = tf.summary.merge_all()\n",
    "                return crf_cost\n",
    "        else:\n",
    "            with tf.variable_scope('softmax_cost'):\n",
    "                cross_entropy_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.tags, logits=logits, name='cross_entropy_cost')\n",
    "                # only compute seq_lengths cost\n",
    "                masked_seq = tf.sequence_mask(lengths=seq_lengths)\n",
    "                cross_entropy_loss = tf.boolean_mask(tensor=cross_entropy_loss, mask=masked_seq)\n",
    "                softmax_cost = tf.reduce_mean(cross_entropy_loss)\n",
    "\n",
    "                tf.summary.scalar(name='cost', tensor=softmax_cost)\n",
    "                tf.summary.histogram(name='histogram_cost', values=softmax_cost)\n",
    "                self.summary_op = tf.summary.merge_all()\n",
    "                return softmax_cost\n",
    "\n",
    "    def optimize(self, cost):\n",
    "        \"\"\"\n",
    "        Adam optimizer with gradient clipping. \n",
    "        \"\"\"\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "            # compute_gradients return: [(gradient_a, variable_a), (gradient_b, variable_b)]\n",
    "            # gradients: (gradient_a, gradient_b)\n",
    "            # variables: (variable_a, variable_b)\n",
    "            gradients, variables = zip(*optimizer.compute_gradients(loss=cost))\n",
    "            clipped_gradients, _ = tf.clip_by_global_norm(t_list=gradients, clip_norm=self.max_gradient_norm)\n",
    "            optimize = optimizer.apply_gradients(grads_and_vars=zip(clipped_gradients, variables), global_step=self.global_step)\n",
    "        return optimize\n",
    "    \n",
    "\n",
    "    def step(self, sess, mini_batch, is_training, keep_prop):\n",
    "        \"\"\"\n",
    "        Run the model one time.\n",
    "        :param sess: tensorflow session.\n",
    "        :param mini_batch: mini_batch data.\n",
    "        :param is_training: flag denotes whether training step or testing step.\n",
    "        :param keep_prop: keep probability of dropout.\n",
    "        :return: statistics of this mini-batch.\n",
    "        \"\"\"\n",
    "        _, tmp_chars, tmp_words, tmp_tags = mini_batch\n",
    "\n",
    "        mini_batch_fd = {}\n",
    "        mini_batch_fd[self.char_inputs] = np.asarray(tmp_chars)\n",
    "        mini_batch_fd[self.word_inputs] = np.asarray(tmp_words)\n",
    "        mini_batch_fd[self.keep_prob] = keep_prop\n",
    "\n",
    "        if is_training:\n",
    "            mini_batch_fd[self.tags] = np.asarray(tmp_tags)\n",
    "            _, mini_batch_global_step, mini_batch_cost, mini_batch_summary = sess.run([self.train_op, self.global_step, self.cost, self.summary_op], feed_dict=mini_batch_fd)\n",
    "            return mini_batch_global_step, mini_batch_cost, mini_batch_summary\n",
    "        else:\n",
    "            seq_length, predictions = sess.run([self.seq_lengths, self.logits], feed_dict=mini_batch_fd)\n",
    "            return seq_length, predictions\n",
    "\n",
    "    def decode(self, logits, seq_length, transition_matrix):\n",
    "        \"\"\"\n",
    "        Decode the best tags via Viterbi algorithm.\n",
    "        :param logits: predicted tag scores.\n",
    "        :param seq_length: actual sequence lengths.\n",
    "        :param transition_matrix: tag transition matrix.\n",
    "        :return: best tags for each character.\n",
    "        \"\"\"\n",
    "        best_tags = []\n",
    "        small = -1000.0\n",
    "        start_logits = np.asarray(a=[[small] * self.num_tags + [0, small]])\n",
    "        end_logits = np.asarray(a=[[small] * self.num_tags + [small, 0]])\n",
    "        # iterate each sequence\n",
    "        for tmp_logits, tmp_length in zip(logits, seq_length):\n",
    "            tmp_logits = tmp_logits[:tmp_length]\n",
    "            padded_logits = np.concatenate([tmp_logits, small * np.ones(shape=[tmp_length, 2])], axis=1)\n",
    "            # final_logits: [seq_len+2, num_tags+2]\n",
    "            final_logits = np.concatenate([start_logits, padded_logits, end_logits], axis=0)\n",
    "            # score: [seq_len, num_tags] matrix\n",
    "            best_tag, _ = tf.contrib.crf.viterbi_decode(score=final_logits, transition_params=transition_matrix)\n",
    "            # 1:len(best_tag)-1 means excludes start and end tags\n",
    "            best_tags.append(best_tag[1:len(best_tag)-1])\n",
    "        return best_tags\n",
    "\n",
    "\n",
    "    def evaluate(self, sess, data_manager, id_to_tag):\n",
    "        \"\"\"\n",
    "        Evaluate the model performance on dev or test data set.\n",
    "        :param sess: tensorflow session.\n",
    "        :param data_manager: dev or test data manager.\n",
    "        :param id_to_tag: convert tag id to tag token.\n",
    "        :return: [character_1 - real tag_1 - predicted tag_1, character_2 - real tag_2 - predicted tag_2, ...]\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        if self.use_crf:\n",
    "            transition_matrix = sess.run(self.transition_matrix)\n",
    "        \n",
    "        for mini_batch in data_manager.iter_batch():\n",
    "            tmp_strings = mini_batch[0]\n",
    "            tmp_tags = mini_batch[-1]\n",
    "            tmp_lengths, tmp_logits = self.step(sess, mini_batch, is_training=False, keep_prop=1.0)\n",
    "            \n",
    "            if self.use_crf:\n",
    "                batch_paths = self.decode(tmp_logits, tmp_lengths, transition_matrix)\n",
    "            else:\n",
    "                batch_paths = sess.run(tf.cast(tf.argmax(tmp_logits, axis=-1), tf.int32))\n",
    "                \n",
    "            for i in range(len(tmp_strings)):\n",
    "                result = []\n",
    "                string = tmp_strings[i][:tmp_lengths[i]]\n",
    "                # real tags\n",
    "                gold = iobes_iob([id_to_tag[int(x)] for x in tmp_tags[i][:tmp_lengths[i]]])\n",
    "                # predicted tags\n",
    "                pred = iobes_iob([id_to_tag[int(x)] for x in batch_paths[i][:tmp_lengths[i]]])\n",
    "                # for each sample in one batch, store the character, real tag and predicted tag\n",
    "                for char, gold, pred in zip(string, gold, pred):\n",
    "                    result.append(' '.join([char, gold, pred]))\n",
    "                # stores the whole data for all mini-batches\n",
    "                results.append(result)\n",
    "        return results\n",
    "\n",
    "    def evaluate_line(self, sess, inputs, id_to_tag):\n",
    "        lengths, scores = self.step(sess, inputs, is_training=False, keep_prop=1.0)\n",
    "        \n",
    "        if self.use_crf:\n",
    "            transition_matrix = sess.run(self.transition_matrix)\n",
    "            batch_paths = self.decode(scores, lengths, transition_matrix)\n",
    "        else:\n",
    "            batch_paths = sess.run(tf.cast(tf.argmax(scores, axis=-1), tf.int32))\n",
    "            \n",
    "        tags = [id_to_tag[idx] for idx in batch_paths[0]]\n",
    "        return result_to_json(inputs[0][0], tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load train.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "from model import BrandsNERModel\n",
    "from conlleval import return_report\n",
    "from utils_train import BatchManager, prepare_dataset, update_tag_scheme\n",
    "from utils_io import clean, make_path, load_sentences, augment_with_pretrained, tag_mapping, char_mapping, load_config, save_config, load_word2vec\n",
    "\n",
    "flags = tf.app.flags\n",
    "\n",
    "# input\n",
    "flags.DEFINE_string('data_dir',          'data',   'Path for training, development, testing and embedding data.')\n",
    "\n",
    "# output\n",
    "flags.DEFINE_string('summary_dir',  'summaries',   'Path for training and testing summaries.')\n",
    "flags.DEFINE_string('ckpt_dir',     'checkpoints',  'Path for saving model checkpoints.')\n",
    "\n",
    "# pre-processing\n",
    "flags.DEFINE_boolean('zeros',            True,       'Replace digits with zero.')\n",
    "flags.DEFINE_boolean('lower',            True,       'Convert character to lower case.')\n",
    "flags.DEFINE_string('tag_schema',     'iobes',    'Tagging schema iobes or iob')\n",
    "\n",
    "# bi-directional lstm + crf model\n",
    "flags.DEFINE_integer('word_dim',            20,        'Embedding dimension for word, 0 if not used.')\n",
    "flags.DEFINE_integer('char_dim',           100,        'Embedding dimension for character.')\n",
    "flags.DEFINE_integer('num_units',          100,        'Number of recurrent units in LSTM cell.')\n",
    "\n",
    "# training\n",
    "flags.DEFINE_float('learning_rate',       0.001,      'Initial learning rate.')\n",
    "flags.DEFINE_float('max_gradient_norm',       5,      'Clip gradients to this norm.')\n",
    "flags.DEFINE_float('batch_size',             20,      'Batch size to use during training.')\n",
    "flags.DEFINE_float('keep_prop',             0.5,      'Initial dropout rate.')\n",
    "flags.DEFINE_boolean('use_crf',            True,      'Use crf layer or softmax layer as the top layer.')\n",
    "flags.DEFINE_integer('num_epoch',           100,      'Number of epochs.')\n",
    "\n",
    "# util\n",
    "flags.DEFINE_boolean('clean',              True,      'Clean all the training-related folders and files.')\n",
    "\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "def create_model(session, Model_class, path, load_vec, config, id_to_char):\n",
    "    \"\"\"\n",
    "    Train the new model or re-use trained model.\n",
    "    \"\"\"\n",
    "    model = Model_class(config)\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir=path)\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        print('Reading model parameters from %s' % ckpt.model_checkpoint_path)\n",
    "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print('Created model with fresh parameters.')\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        # assign character embeddings\n",
    "        emb_weights = session.run(model.char_embeddings.read_value())\n",
    "        emb_weights = load_vec(config['character_embedding_file'], id_to_char, config['char_dim'], emb_weights)\n",
    "        session.run(model.char_embeddings.assign(emb_weights))\n",
    "    return model\n",
    "\n",
    "\n",
    "def train(training_file_name, dev_file_name, test_file_name, maps_file_name, character_embedding_file_name, config_file_name):\n",
    "    \"\"\"\n",
    "    Train main entrance.\n",
    "    \"\"\"\n",
    "    training_file = os.path.join(FLAGS.data_dir, training_file_name)\n",
    "    dev_file = os.path.join(FLAGS.data_dir, dev_file_name)\n",
    "    test_file = os.path.join(FLAGS.data_dir, test_file_name)\n",
    "    maps_file = os.path.join(FLAGS.data_dir, maps_file_name)\n",
    "    embedding_file = os.path.join(FLAGS.data_dir, character_embedding_file_name)\n",
    "    config_file = os.path.join(FLAGS.data_dir, config_file_name)\n",
    "\n",
    "    # load data sets\n",
    "    # brands.train, dev, test tagging schema: IOB\n",
    "    # train_sentences format: [[['a', 'B-SHOE'], ['b', 'I-SHOE'], ['c', I-SHOE], ['d', 'O'], ...], [next training example data]]\n",
    "    train_sentences = load_sentences(training_file, FLAGS.zeros)\n",
    "    dev_sentences = load_sentences(dev_file, FLAGS.zeros)\n",
    "    test_sentences = load_sentences(test_file, FLAGS.zeros)\n",
    "\n",
    "    # Use selected tagging scheme (IOB / IOBES)\n",
    "    # train_sentences format: [[['a', 'B-SHOE'], ['b', 'I-SHOE'], ['c', E-SHOE], ['d', 'O'], ...], [next training example data]]\n",
    "    update_tag_scheme(train_sentences, FLAGS.tag_schema)\n",
    "    update_tag_scheme(test_sentences, FLAGS.tag_schema)\n",
    "\n",
    "    # create maps.pkl if not exist\n",
    "    # maps.pkl contains: char_to_id, id_to_char, tag_to_id, id_to_tag\n",
    "    if not os.path.isfile(maps_file):\n",
    "        print('create map file')\n",
    "        # create dictionary for each character\n",
    "        dict_chars_train = char_mapping(train_sentences, FLAGS.lower)[0]\n",
    "        # update dictionary by add the characters in embedding files or test data set\n",
    "        dict_chars, char_to_id, id_to_char = augment_with_pretrained(dict_chars_train.copy(), embedding_file, list(itertools.chain.from_iterable([[w[0] for w in s] for s in dev_sentences])))\n",
    "        # Create a dictionary and a mapping for tags\n",
    "        _t, tag_to_id, id_to_tag = tag_mapping(train_sentences)\n",
    "        # pickle data\n",
    "        with open(maps_file, 'wb') as f:\n",
    "            pickle.dump([char_to_id, id_to_char, tag_to_id, id_to_tag], f)\n",
    "    else:\n",
    "        print('load map file')\n",
    "        with open(maps_file, 'rb') as f:\n",
    "            char_to_id, id_to_char, tag_to_id, id_to_tag = pickle.load(f)\n",
    "\n",
    "    # convert character, tag, word segmentation to id\n",
    "    train_data = prepare_dataset(train_sentences, char_to_id, tag_to_id, FLAGS.lower)\n",
    "    dev_data = prepare_dataset(dev_sentences, char_to_id, tag_to_id, FLAGS.lower)\n",
    "    test_data = prepare_dataset(test_sentences, char_to_id, tag_to_id, FLAGS.lower)\n",
    "    print('%i / %i / %i sentences in train / dev / test.' % (len(train_data), len(dev_data), len(test_data)))\n",
    "\n",
    "    # prepare mini-batch data\n",
    "    train_manager = BatchManager(train_data, FLAGS.batch_size)\n",
    "    dev_manager = BatchManager(dev_data, 100)\n",
    "    test_manager = BatchManager(test_data, 100)\n",
    "\n",
    "    # make path for store summary and model if not exist\n",
    "    make_path(FLAGS)\n",
    "\n",
    "    if os.path.isfile(config_file):\n",
    "        config = load_config(config_file)\n",
    "    else:\n",
    "        config = OrderedDict()\n",
    "        config['num_chars'] = len(char_to_id)\n",
    "        config['char_dim'] = FLAGS.char_dim\n",
    "        config['num_tags'] = len(tag_to_id)\n",
    "        config['word_dim'] = FLAGS.word_dim\n",
    "        config['num_units'] = FLAGS.num_units\n",
    "        config['batch_size'] = FLAGS.batch_size\n",
    "        config['character_embedding_file'] = os.path.join(FLAGS.data_dir, character_embedding_file_name)\n",
    "        config['max_gradient_norm'] = FLAGS.max_gradient_norm\n",
    "        config['keep_prop'] = FLAGS.keep_prop\n",
    "        config['learning_rate'] = FLAGS.learning_rate\n",
    "        config['zeros'] = FLAGS.zeros\n",
    "        config['lower'] = FLAGS.lower\n",
    "        config['use_crf'] = FLAGS.use_crf\n",
    "        save_config(config, config_file)\n",
    "\n",
    "    # config parameters for the tf.Session\n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    # number of mini-batches per epoch\n",
    "    steps_per_epoch = train_manager.len_data\n",
    "\n",
    "    with tf.Session(config=tf_config) as sess:\n",
    "        # train_writer = tf.summary.FileWriter(logdir=os.path.join(FLAGS.summary_dir, 'train'), graph=sess.graph)\n",
    "        # test_writer = tf.summary.FileWriter(logdir=os.path.join(FLAGS.summary_dir, 'test'), graph=sess.graph)\n",
    "        train_writer = tf.summary.FileWriter(logdir=FLAGS.summary_dir, graph=sess.graph)\n",
    "        model = create_model(sess, BrandsNERModel, FLAGS.ckpt_dir, load_word2vec, config, id_to_char)\n",
    "        loss = []\n",
    "        for i in range(FLAGS.num_epoch):\n",
    "            for mini_batch in train_manager.iter_batch(shuffle=True):\n",
    "                global_step, mini_batch_cost, mini_batch_summary = model.step(sess, mini_batch, is_training=True, keep_prop=FLAGS.keep_prop)\n",
    "                train_writer.add_summary(summary=mini_batch_summary, global_step=global_step)\n",
    "                loss.append(mini_batch_cost)\n",
    "                if global_step % 100 == 0:\n",
    "                    print('iteration:{} step:{}/{}, NER loss:{:>9.6f}'.format(i+1, global_step%steps_per_epoch, steps_per_epoch, np.mean(loss)))\n",
    "                    loss = []\n",
    "            # evaluate the model on development data\n",
    "            best = evaluate(sess, model, 'dev', dev_manager, id_to_tag)\n",
    "            # if have better dev F1 score until now, then save the model\n",
    "            if best:\n",
    "                model.saver.save(sess=sess, save_path=os.path.join(FLAGS.ckpt_dir, 'Brands_ner.ckpt'), global_step=model.global_step.eval())\n",
    "            # report the test F1 score\n",
    "            evaluate(sess, model, 'test', test_manager, id_to_tag)\n",
    "\n",
    "\n",
    "def evaluate(sess, model, name, data, id_to_tag):\n",
    "    print('====================== evaluate:{}'.format(name))\n",
    "\n",
    "    # ner_results contains 'character - real tag - predicted tag' for all samples in 'data'\n",
    "    ner_results = model.evaluate(sess, data, id_to_tag)\n",
    "    eval_lines = test_ner(ner_results, FLAGS.data_dir)\n",
    "\n",
    "    for line in eval_lines:\n",
    "        print(line)\n",
    "    f1 = float(eval_lines[1].strip().split()[-1])\n",
    "\n",
    "    if name == 'dev':\n",
    "        best_test_f1 = model.best_dev_f1.eval()\n",
    "        if f1 > best_test_f1:\n",
    "            sess.run(model.best_dev_f1.assign(f1))\n",
    "            print('new best dev f1 score:{:>.3f}'.format(f1))\n",
    "        return f1 > best_test_f1\n",
    "    elif name == 'test':\n",
    "        best_test_f1 = model.best_test_f1.eval()\n",
    "        if f1 > best_test_f1:\n",
    "            sess.run(model.best_test_f1.assign(f1))\n",
    "            print('new best test f1 score:{:>.3f}'.format(f1))\n",
    "        return f1 > best_test_f1\n",
    "\n",
    "def test_ner(results, path):\n",
    "    \"\"\"\n",
    "    Report the performance.\n",
    "    \"\"\"\n",
    "    output_file = os.path.join(path, 'Brands_ner_predict.utf8')\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        to_write = []\n",
    "        for block in results:\n",
    "            for line in block:\n",
    "                to_write.append(line + '\\n')\n",
    "            to_write.append('\\n')\n",
    "\n",
    "        f.writelines(to_write)\n",
    "    eval_lines = return_report(output_file)\n",
    "    return eval_lines\n",
    "\n",
    "def main(_):\n",
    "    if FLAGS.clean:\n",
    "        clean(FLAGS, 'maps.pkl', 'BrandsNERModel.config', 'Brands_ner_predict.utf8')\n",
    "    train('brands.train', 'brands.dev', 'brands.test', 'maps.pkl', 'wiki_100.utf8', 'BrandsNERModel.config')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.app.run(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
